{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/paddle-ocr-kie-sdmgr-loss-and-evaluation","result":{"data":{"markdownRemark":{"html":"<p>在前面的几篇文章中，我们结合代码介绍了关键信息提取(KIE)任务网络<a href=\"https://arxiv.org/abs/2103.14470v1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SDMGR</a>(Spatial Dual-Modality Graph Reasoning for Key Information Extraction)的整个前向计算过程，包含了处理图片信息的主干网络U-Net，处理文字信息的LSTM，以及特征融合的图神经网络部分。今天就让我们继续看看SDMGR的损失函数以及模型评估部分吧。</p>\n<p><span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <a class='gatsby-resp-image-link' href='/static/9f2841aa6e29db8849739d5cc5f853a4/56873/sdmgr-net.jpg' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 46.28571428571429%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAEDAgX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHt6mygg//EABcQAAMBAAAAAAAAAAAAAAAAAAABEEH/2gAIAQEAAQUC0VR//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhAAAwAAAAAAAAAAAAAAAAAAACAx/9oACAEBAAY/Air/AP/EABwQAAEEAwEAAAAAAAAAAAAAAAABESExEEFhof/aAAgBAQABPyFpT6PwUlXNrip//9oADAMBAAIAAwAAABBwD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB0QAAICAQUAAAAAAAAAAAAAAAERACExEGFxofH/2gAIAQEAAT8QQEJ2dijeEdkWyIO/TLzP/9k='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='sdmgr-net' title='sdmgr-net' src='/static/9f2841aa6e29db8849739d5cc5f853a4/29d31/sdmgr-net.jpg' srcset='/static/9f2841aa6e29db8849739d5cc5f853a4/e52aa/sdmgr-net.jpg 175w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/70ebb/sdmgr-net.jpg 350w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/29d31/sdmgr-net.jpg 700w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/9ecec/sdmgr-net.jpg 1050w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/d165a/sdmgr-net.jpg 1400w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/56873/sdmgr-net.jpg 2332w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<h1>1. 损失函数</h1>\n<p>损失函数部分的代码位于<code class=\"language-text\">ppocr/losses/kie_sdmgr_loss.py</code>。我们先来看一下损失函数的前向计算：</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">,</span> batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># node_preds.shape = [node_num, class_num], 即每个节点的分类预测值</span>\n    <span class=\"token comment\"># edge_preds.shape = [edge_num, 2]， 即每条边的分类预测值</span>\n    node_preds<span class=\"token punctuation\">,</span> edge_preds <span class=\"token operator\">=</span> pred\n    <span class=\"token comment\"># gts.shape = [batch_size, 300, 300], 即每个节点真实的分类</span>\n    <span class=\"token comment\"># tag.shape = [batch_size. 2], 对每个batch，tag储存了两个信息：[节点数, 最长文字长度]</span>\n    gts<span class=\"token punctuation\">,</span> tag <span class=\"token operator\">=</span> batch<span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> batch<span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">]</span>\n    <span class=\"token comment\"># 将真实的节点标签从300 * 300的容器中拿出，形成形状为[batch_size, node_num, node_num + 1]的标签张量。</span>\n    <span class=\"token comment\"># 其中每个batch第一列[node_num, 1]为节点自身分类，其余列组成的[node_num, node_num]矩阵为节点两两之间边的分类。</span>\n    gts <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>pre_process<span class=\"token punctuation\">(</span>gts<span class=\"token punctuation\">,</span> tag<span class=\"token punctuation\">)</span>\n    node_gts<span class=\"token punctuation\">,</span> edge_gts <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> gt <span class=\"token keyword\">in</span> gts<span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># 将节点分类的标签与边分类的标签分别存放</span>\n        node_gts<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>gt<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        edge_gts<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>gt<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    node_gts <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>node_gts<span class=\"token punctuation\">)</span>\n    edge_gts <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>edge_gts<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># 过滤标签为ignore class的节点标签，默认为第0个class</span>\n    node_valids <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>nonzero<span class=\"token punctuation\">(</span>node_gts <span class=\"token operator\">!=</span> self<span class=\"token punctuation\">.</span>ignore<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    edge_valids <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>nonzero<span class=\"token punctuation\">(</span>edge_gts <span class=\"token operator\">!=</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># 对节点做CrossEntropyLoss</span>\n    loss_node <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>loss_node<span class=\"token punctuation\">(</span>node_preds<span class=\"token punctuation\">,</span> node_gts<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># 对边做CrossEntropyLoss</span>\n    loss_edge <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>loss_edge<span class=\"token punctuation\">(</span>edge_preds<span class=\"token punctuation\">,</span> edge_gts<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># 总loss为节点和边loss的加权和，默认权重都为1</span>\n    loss <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>node_weight <span class=\"token operator\">*</span> loss_node <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>edge_weight <span class=\"token operator\">*</span> loss_edge\n    <span class=\"token keyword\">return</span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">(</span>\n        loss<span class=\"token operator\">=</span>loss<span class=\"token punctuation\">,</span>\n        loss_node<span class=\"token operator\">=</span>loss_node<span class=\"token punctuation\">,</span>\n        loss_edge<span class=\"token operator\">=</span>loss_edge<span class=\"token punctuation\">,</span>\n        acc_node<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>accuracy<span class=\"token punctuation\">(</span>\n            paddle<span class=\"token punctuation\">.</span>gather<span class=\"token punctuation\">(</span>node_preds<span class=\"token punctuation\">,</span> node_valids<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            paddle<span class=\"token punctuation\">.</span>gather<span class=\"token punctuation\">(</span>node_gts<span class=\"token punctuation\">,</span> node_valids<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        acc_edge<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>accuracy<span class=\"token punctuation\">(</span>\n            paddle<span class=\"token punctuation\">.</span>gather<span class=\"token punctuation\">(</span>edge_preds<span class=\"token punctuation\">,</span> edge_valids<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            paddle<span class=\"token punctuation\">.</span>gather<span class=\"token punctuation\">(</span>edge_gts<span class=\"token punctuation\">,</span> edge_valids<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>相对而言，SDMGR的损失函数是比较简单的。对于图神经网络输出的节点与边的分类，各自与标签做<a href=\"https://en.wikipedia.org/wiki/Cross_entropy\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">交叉熵损失</a>即可。而总损失是节点与边两方面损失的加权和，这两个权重就都是超参数了，可以自由调整。结合现阶段的代码实现和实际训练来看，标签数据里并未包含边信息的分类时，直接将边损失的权重置0，只看节点损失即可。</p>\n<h1>2. 模型评估</h1>\n<p>模型评估对应的代码位置为<code class=\"language-text\">ppocr/metrics/kie_metric.py</code>。</p>\n<p>SDMGR的评估采用了<a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">F1 score</a>，即准确率(precision)与召回率(recall)的调和平均数(harmonic mean)，这也是相当常用的模型评估标准了。我们直接来看最核心的代码：</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">compute_f1_score</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> preds<span class=\"token punctuation\">,</span> gts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># 那些不参与模型评估的类们</span>\n    ignores <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">12</span><span class=\"token punctuation\">,</span> <span class=\"token number\">14</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16</span><span class=\"token punctuation\">,</span> <span class=\"token number\">18</span><span class=\"token punctuation\">,</span> <span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">22</span><span class=\"token punctuation\">,</span> <span class=\"token number\">24</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">]</span>\n    <span class=\"token comment\"># 预测的第二个维度形状，也就是预测的类数</span>\n    C <span class=\"token operator\">=</span> preds<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n    <span class=\"token comment\"># 从0到类数生成各个类的index，但是要剥离上述不参与评估的类</span>\n    classes <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token builtin\">sorted</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>ignores<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># gts.shape = [node_num]，是各个节点真实分类的index，然后各个index乘以了类数</span>\n    <span class=\"token comment\"># preds.argmax(1).shape = [node_num]，是经过网络预测后各个节点的分类</span>\n    <span class=\"token comment\"># hist.shape = [class_num, class_num]</span>\n    <span class=\"token comment\"># 如果分类正确，则该节点对应位置上的加和数字该类别的(class_num + 1)倍。该值落在了hist矩阵的对角线上</span>\n    <span class=\"token comment\"># 若分类错误，则加和的数字范围为 （class_num * class_index, class_num * (class_index + 1)), 落在hist矩阵的第class_index行</span>\n    hist <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>bincount<span class=\"token punctuation\">(</span>\n        <span class=\"token punctuation\">(</span>gts <span class=\"token operator\">*</span> C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token string\">'int64'</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> preds<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> minlength<span class=\"token operator\">=</span>C\n        <span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token string\">'float32'</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># hist的对角线diag，即各个分类的ture positive</span>\n    diag <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>diag<span class=\"token punctuation\">(</span>hist<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># hist.sum(1): hist矩阵按列求和，即该类所有的 ture positive + false nagative</span>\n    recalls <span class=\"token operator\">=</span> diag <span class=\"token operator\">/</span> hist<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>clip<span class=\"token punctuation\">(</span><span class=\"token builtin\">min</span><span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># hist.sum(0)：hist矩阵按行求和，即该类所有的positive（ture positive + false positive）</span>\n    precisions <span class=\"token operator\">=</span> diag <span class=\"token operator\">/</span> hist<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>clip<span class=\"token punctuation\">(</span><span class=\"token builtin\">min</span><span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    f1 <span class=\"token operator\">=</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> recalls <span class=\"token operator\">*</span> precisions <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span>recalls <span class=\"token operator\">+</span> precisions<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>clip<span class=\"token punctuation\">(</span><span class=\"token builtin\">min</span><span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">8</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> f1<span class=\"token punctuation\">[</span>classes<span class=\"token punctuation\">]</span></code></pre></div>\n<p>这里有两个值得关注的地方：</p>\n<p>第一点，计算F1分数时，会剔除某些类。对应WildReceipt数据集，这里hardcode了一个ignore列表，该列表包含了数据集中分类为<code class=\"language-text\">Ignore</code>，<code class=\"language-text\">Other</code>,以及所有的<code class=\"language-text\">Key</code>分类。这表明了关键信息提取任务实际关心的是票据或文档中那些<code class=\"language-text\">Value</code>的准确度，比如消费金额啊，税额数字，而不是图像中信息的标题。如果你想将SDMGR应用在其他数据集上，一定要记得评估模型时要将hard code<code class=\"language-text\">ignores</code>数组替换成数据集对用的分类。</p>\n<p>第二点，这里计算F1分数的过程相当巧妙。首先将所有的分类index乘以分类数，然后加上了预测的分类结果，形成了一个<code class=\"language-text\">[node_num]</code>形状的list，然后对这个list中各个值求<code class=\"language-text\">bincount</code>，即计算list中各个数字出现的次数。因为这个list中最大的数也不会超过<code class=\"language-text\">(class_num * class_num)</code>，计算完<code class=\"language-text\">bincount</code>后的数组就可以reshape成一个<code class=\"language-text\">[class_num, class_num]</code>的<code class=\"language-text\">hist</code>矩阵了。如果分类正确，那么该class对应的数字为<code class=\"language-text\">(class_num * class_index + class_index)</code>，正好落在<code class=\"language-text\">hist</code>矩阵的对角线上。也就意味着将对角线单独拿出，就正好对应了各个class被正确分类的次数，即<code class=\"language-text\">ture positive</code>，这是计算recall与precisions的分母。</p>\n<p>如果预测分类错误，假设应该被分类为<code class=\"language-text\">class_a</code>的节点被错误分类为了<code class=\"language-text\">class_b</code>则加和的数字范围为<code class=\"language-text\">class_num * class_a + class_b</code>, 落在hist矩阵的第<code class=\"language-text\">class_a</code>行<code class=\"language-text\">class_b</code>列的位置，也就是说hist矩阵的第n行加和，即被分类为为<code class=\"language-text\">class_index</code>为n的总数，也就是<code class=\"language-text\">ture positive + false positive</code></p>\n<p>而hist矩阵的第n列，对应了第n类分类正确的数量，加上了本因被分为第n类缺分类错误的数量，即<code class=\"language-text\">ture positive + false nagative</code></p>\n<p>结合上述hist矩阵的对角线，按行求和以及按列求和，就可以轻松算出各个分类的recalls与precisions，进而求得F1 score了。</p>\n<p>这个过程结合一个例子，使用图像的方法更容易理解： 假设现有5个类，index分别为<code class=\"language-text\">[0, 1, 2, 3, 4]</code>。那么hist矩阵将是一个5 * 5的矩阵</p>\n<p><span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 393px; '>\n      <a class='gatsby-resp-image-link' href='/static/413ad933c19c8632e5f53c7b7f581f42/02e12/grid1.jpg' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 97.14285714285715%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAH3JcnRAgaB/8QAFxABAQEBAAAAAAAAAAAAAAAAAAEQQf/aAAgBAQABBQLI6i5H/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAFBABAAAAAAAAAAAAAAAAAAAAMP/aAAgBAQAGPwIf/8QAGxAAAgIDAQAAAAAAAAAAAAAAAEEBMRARIWH/2gAIAQEAAT8hne2d9wkJVKGLYof/2gAMAwEAAgADAAAAENDAPP/EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8QH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8QH//EAB4QAAICAgIDAAAAAAAAAAAAAAERADEhQXGBUZHB/9oACAEBAAE/ED7QGI0d0TY1CKILwd8RbKxYfmY+hlUV6C0ICwxudn3LOZ//2Q=='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='hist_empty' title='hist_empty' src='/static/413ad933c19c8632e5f53c7b7f581f42/02e12/grid1.jpg' srcset='/static/413ad933c19c8632e5f53c7b7f581f42/e52aa/grid1.jpg 175w,\n/static/413ad933c19c8632e5f53c7b7f581f42/70ebb/grid1.jpg 350w,\n/static/413ad933c19c8632e5f53c7b7f581f42/02e12/grid1.jpg 393w' sizes='(max-width: 393px) 100vw, 393px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<p>假设经过网络的前向计算，将某个标号为第2类的节点正确分类为2，那么代码中</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">(</span>gts <span class=\"token operator\">*</span> C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token string\">'int64'</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> preds<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>该值的计算结果就是2 * 5 + 2 = 12。在hist矩阵中从左上角开始第13格(第一格从0开始)加1。从下图可以很明确地看出这格就是hist矩阵中第2行第2列，正好位于对角线上</p>\n<p><span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 393px; '>\n      <a class='gatsby-resp-image-link' href='/static/c40bb41056006a7ffb8a940dfbaa5b94/02e12/grid2.jpg' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 97.14285714285715%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAH3JcnRAgaB/8QAFxABAQEBAAAAAAAAAAAAAAAAAAEQQf/aAAgBAQABBQLI6i5H/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAFBABAAAAAAAAAAAAAAAAAAAAMP/aAAgBAQAGPwIf/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAERQRAxYYH/2gAIAQEAAT8hcyS+4MV46LG2ND//2gAMAwEAAgADAAAAENDAPP/EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8QH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8QH//EAB0QAAICAwEBAQAAAAAAAAAAAAERADEhQXFRgZH/2gAIAQEAAT8QPabWJgbqTYhlFvB3yLZWNP2IeOmVRXoLQgLDG59P7LOz/9k='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='hist_correct' title='hist_correct' src='/static/c40bb41056006a7ffb8a940dfbaa5b94/02e12/grid2.jpg' srcset='/static/c40bb41056006a7ffb8a940dfbaa5b94/e52aa/grid2.jpg 175w,\n/static/c40bb41056006a7ffb8a940dfbaa5b94/70ebb/grid2.jpg 350w,\n/static/c40bb41056006a7ffb8a940dfbaa5b94/02e12/grid2.jpg 393w' sizes='(max-width: 393px) 100vw, 393px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<p>如果某个节点标号本应为2，但被错误分类为3，那么就应该在hist矩阵中第2 * 5 + 3 = 13格子上加1：</p>\n<p><span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 393px; '>\n      <a class='gatsby-resp-image-link' href='/static/4a591ba383792ebfc1e581ea69b2e107/02e12/grid3.jpg' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 97.14285714285715%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAH3JcnRAgaB/8QAFxABAQEBAAAAAAAAAAAAAAAAAAEQQf/aAAgBAQABBQLI6i5H/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAFBABAAAAAAAAAAAAAAAAAAAAMP/aAAgBAQAGPwIf/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAERQRAxYYH/2gAIAQEAAT8hcyS+4MV46LG2ND//2gAMAwEAAgADAAAAENDAPP/EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8QH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8QH//EAB0QAAICAwEBAQAAAAAAAAAAAAERADEhQXFRgZH/2gAIAQEAAT8QPabWJgbqTYhlFvB3yLZWNP2IeOmVRXoLQgLDG59P7LOz/9k='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='hist_correct' title='hist_correct' src='/static/4a591ba383792ebfc1e581ea69b2e107/02e12/grid3.jpg' srcset='/static/4a591ba383792ebfc1e581ea69b2e107/e52aa/grid3.jpg 175w,\n/static/4a591ba383792ebfc1e581ea69b2e107/70ebb/grid3.jpg 350w,\n/static/4a591ba383792ebfc1e581ea69b2e107/02e12/grid3.jpg 393w' sizes='(max-width: 393px) 100vw, 393px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<p>该格是hist矩阵中第2行第3列。现在我们可以很简单地观察出，hist矩阵中第n行上所有的数据都代表了该节点的真实标号是第n类，而第m列上的数据代表了该节点经过网络前向计算后，被分类成了第m列。通过加和对角线，各行以及各列的总数，我们就能轻松计算出F1 score了。</p>\n<h1>3.总结</h1>\n<p>整个PaddleOCR实现的SDMGR网络代码解析到这就告一段落了。在<a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Transformer</a>架构出现后的这几年里，在NLP与CV领域的应用可谓是大杀四方，大有取代CNN与RNN之势。也正是因为Transformer架构的通用性和强劲的性能，最近类似于<a href=\"https://arxiv.org/abs/2102.03334\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ViLT</a>的基于Transformer的多模态模型可谓是百花齐放。而在此背景下，<code class=\"language-text\">SDMGR</code>仍然创新性地使用了图神经网络，融合了CNN与RNN处理地特征，结合对关键信息提取问题的诸多归纳偏置，以远小于Transformer的计算代价完成了对文档类图片的多模态KIE任务。虽然不及Transformer架构模型那用的通用，但其相同性能下简单的结构和较小的计算量使得在工业应用上也更加方便。</p>\n<p>另外相当推荐大家阅读一下<code class=\"language-text\">ViLT</code>的论文，除了贡献了性能优越的多模态模型外，将这篇论文当作近几年的多模态文献综述也相当不错。也可以看看<a href=\"https://www.bilibili.com/video/BV14r4y1j74y\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">朱毅老师在B站上的视频</a></p>\n<h3>参考链接</h3>\n<ol>\n<li><a href=\"https://en.wikipedia.org/wiki/Cross_entropy\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://en.wikipedia.org/wiki/Cross_entropy</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://en.wikipedia.org/wiki/F-score</a></li>\n<li><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/abs/1706.03762</a></li>\n<li><a href=\"https://arxiv.org/abs/2102.03334\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/abs/2102.03334</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV14r4y1j74y\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.bilibili.com/video/BV14r4y1j74y</a></li>\n</ol>","excerpt":"在前面的几篇文章中，我们结合代码介绍了关键信息提取(KIE)任务网络SDMGR(Spatial Dual-Modality Graph Reasoning for Key Information Extraction)的整个前向计算过程，包含了处理图片信息的主干网络U-Net，处理文字信息的LSTM，以及特征融合的图神经网络部分。今天就让我们继续看看SDMGR的损失函数以及模型评估部分吧。  1. 损失函数 损失函数部分的代码位于。我们先来看一下损失函数的前向计算： 相对而言，SDMGR…","frontmatter":{"date":"August 19, 2022","path":"/paddle-ocr-kie-sdmgr-loss-and-evaluation","title":"关键信息提取网络SDMGR代码详解(4): 损失函数与模型评估"}}},"pageContext":{}},"staticQueryHashes":["1176552510","3649515864","63159454","846684790"]}