{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/aws-athena-s3-data-lake-architecture","result":{"data":{"markdownRemark":{"html":"<p>机器学习和大数据分析技术在过去十年内高速发展，到今天已经成为了现代化的服务与应用不可或缺的一部分。而搭建一套数据湖系统则是开始将机器学习与数据分析带到项目中的第一步。在这几篇文章里，我们将会学习到如何使用<code class=\"language-text\">AWS Athena</code>服务与<code class=\"language-text\">S3</code>文件存储搭建一套完整的数据湖解决方案。</p>\n<h1>1. AWS的数据湖</h1>\n<p>在AWS的这篇<a href=\"https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">文章</a>中，阐述了<code class=\"language-text\">数据湖</code>与<code class=\"language-text\">数据仓库</code>的主要区别。简单地概括一下即：</p>\n<ul>\n<li>“数据仓库”一般指存储结构化业务数据的系统，数据的Schema在数据存储时就已经确定(<code class=\"language-text\">schema-on-write</code>)，这种系统经常用来做批量的业务数据分析以及可视化。比较典型的例子就是<a href=\"https://aws.amazon.com/redshift/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">AWS Redshift</a>列式存储数据库，非常适合用来做业务的<code class=\"language-text\">OLAP</code>任务。</li>\n<li>“数据湖”则一般指不仅存储了业务上的关系型数据，还可以存储从移动设备，IoT设备或社交网络中获取的非关系型数据或指标数据(Metrics)，在储存到数据湖的时候并不指定数据的Schema，而仅在数据被读取的时候使用某种Schema去序列化数据湖中的数据(<code class=\"language-text\">schema-on-read</code>)，或者衔接使用<code class=\"language-text\">ETL</code>(Extract, transform, load)任务来清洗，分类数据。这样就大大降低了数据采集和存储端的复杂度，并最大限度地保留了数据的灵活性。比如同一份数据，即可以通过Athena使用SQL以及一个固定的Schema读取，也可以通过AWS Glue ETL Job转化成其他的数据结构。</li>\n</ul>\n<p>从上面的描述可以看出，数据湖是一系列数据服务的中心。而从架构上，AWS数据湖是这样的:\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <a class='gatsby-resp-image-link' href='/static/690142f2a18f836711b8789b2756a000/f0685/data-lake-arch.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAACgElEQVQ4y5VTTU8TURSdH2bcaWJiwj8wLpQFCxNjSEwUY3RhFEg0JixIdEHVuLPpAhAkDcTERKACsYFSKq3Qzldn5s3Mm4/3Od7XodCGhfA2c9/ce9495573tOz8kvIskGpBGCbh1NKzmfJ0vhUqk2nZ/5cUXKLIvzM7+uDDOKBOE+fAQkhCTppDF0rhw7mAnWG6lo1UCZcREpwOgqWEkmB+0bs/TjY2Vby0Yt++S8urKnbiwpN2cUqBa5v+zL2/299ibbBnkmWduY+16zeDte/Q2iqW9m6MoNI874EnxyrvJg6g8PdP++Xor81lV+t3VVpSwuIgcJqtBGP4iRynvrXju25KgHym65ZpdiEghFomiuN0SHOSklMJkiu1+Q6qcURmC1szhS1Klf48oQkpcRR7yBdCpCmVnEEirMzbr67QaqknhwPY9fDzyfKjFytJyvKxKqsCjHXLOjhswRHQGSYLCt31r/Wn1/DOYu4U/Mc4ajb1RuOYqvn3rXIRcjwvxBEIo4z17oAMAr9Rq3melxelhIQ+Kb6xVufwoEwNEI6LAAxMANbajUpvrfae4qb8RWaapFGa6Mfe9Fjl/eN9zkTWF62dXUQFFj8WjImR6vqCJzKG/SCoVwLkhVECovb/HDVanZ6sfmegBNMKQhwnBE4wO37x84atK3pAp3XURkEYE8oSkixV2Fp1iDYo7RimaXdhXIILQknH0imjQxYyjg139+rD9q3X6k2c0oYJdR3Pdlwh5fl3deIz4yHy9z8t17+sDk37Aq8qS5KUCylizONwiPZFwLR3rVCzFbY7lwYzzpUOwmXvel0ODH6nnLnVw+52XV4WnDM3DF03jEGf/wHvL2sy7mCihwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='data-lake-arch' title='data-lake-arch' src='/static/690142f2a18f836711b8789b2756a000/8c557/data-lake-arch.png' srcset='/static/690142f2a18f836711b8789b2756a000/4edbd/data-lake-arch.png 175w,\n/static/690142f2a18f836711b8789b2756a000/13ae7/data-lake-arch.png 350w,\n/static/690142f2a18f836711b8789b2756a000/8c557/data-lake-arch.png 700w,\n/static/690142f2a18f836711b8789b2756a000/f0685/data-lake-arch.png 835w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<p>在图中，整个数据湖及其下游架构包含了相当多的服务，而我们这次只需要了解构成数据湖的最基本服务:</p>\n<ul>\n<li>AWS S3: 文件存储系统，S3之于数据湖就好像<code class=\"language-text\">HDFS</code>之于<code class=\"language-text\">Hadoop</code>。在AWS数据湖概念中，基本上S3是所有数据存放的地方，也就是“湖”这个概念本身。而将数据放入“湖”中，即是简单地将文件通过API存入S3桶。</li>\n<li>AWS Athena: 在有了“湖”后，下一个需要解决的问题是如何从湖中获取数据。Athena基于<a href=\"https://trino.io/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Trino/Presto</a>引擎和<a href=\"https://spark.apache.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Apache Spark</a>框架实现了通过SQL来查询数据湖中的数据。</li>\n<li>AWS Glue: 包含在Glue中的服务非常多，而其中最关键的部分是<code class=\"language-text\">Data catalog</code>。如上文所提到的，数据湖是一个Schema-on-read系统，而AWS Glue就扮演了这个\"Schema\"的角色。当使用Athena查询数据时，需要选择定义在Glue中的Schema，将类似JSON文件的无Schema数据序列化后读取出来。</li>\n</ul>\n<p><span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <a class='gatsby-resp-image-link' href='/static/53ecbc1902ce3792588177f4b736e87f/7eab7/athena-glue.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 34.85714285714286%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABHUlEQVQoz1VS2U7EMAzs/38bD0jwAGhRWXa7hTa9tkfSOI6xnRZKJEuJnRmPJ8kCIkl4DxqIkWJMgRL7We5NA4W5J7QD54Pmh/tIbddH2UMIMROQ95764a5FZKAU/1baCyEOhqLJCWejhIJtuo6q2kTBhYBMyArcuiqhtY6TQTpJkeyKtDjUswDAOZqvLwTeaR0ARB2ZtosrcyRC7rJYq4ROk0FDdJ1vPT29FmxJUvpV1fTw+Ey1aZINnBXsMI46ho4shWle6PNS0Hdl1M995FtZ0tvplEbmnGWF4ziRc6sqxs1D07QHDzcCHXPb74TXoqT3/KyKj2uvC3H+cZE7USbkBjFTf/69Kv6GAHZfxWvcfsTxDkAgD6CP4iHEH42OIpzDrP4HAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='athena-glue' title='athena-glue' src='/static/53ecbc1902ce3792588177f4b736e87f/8c557/athena-glue.png' srcset='/static/53ecbc1902ce3792588177f4b736e87f/4edbd/athena-glue.png 175w,\n/static/53ecbc1902ce3792588177f4b736e87f/13ae7/athena-glue.png 350w,\n/static/53ecbc1902ce3792588177f4b736e87f/8c557/athena-glue.png 700w,\n/static/53ecbc1902ce3792588177f4b736e87f/e996b/athena-glue.png 1050w,\n/static/53ecbc1902ce3792588177f4b736e87f/2cefc/athena-glue.png 1400w,\n/static/53ecbc1902ce3792588177f4b736e87f/7eab7/athena-glue.png 2360w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<p>接下来我们着重介绍一下Athena以及Glue。</p>\n<h1>2. AWS Athena</h1>\n<p>AWS Athena在整个流水线中中起到了数据访问接口的作用：\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <a class='gatsby-resp-image-link' href='/static/d46958f71cb7723842d6e83cd6b75861/2dc7d/athena-arch.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 62.857142857142854%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAABYlAAAWJQFJUiTwAAABdElEQVQoz3VSaXOCQAzl//88ZzqtRRGByimLcrlHlr49QPqhmRBjNsfblw30YkRrTfj0oo3v1UV8fFUn+OVCBULKaZ5fr5e2ob2sff8EuZBcCDhCqmAYxp9bUVaNlFIRQe0kD8ZY928FmJdVludKkVQqGKdpHKdnP8DimMhW7yevFn3hACVS/WRgfjx7zoWrMhapUlBzU02mWEqPzBOiNSrjaxrF12magTPAdft+UFa2YoIvxZIclvpbawCUZNFgTF5URVkz9vDFrENwEJYGstdGBy6pTC9lEipaUK3et/FiYHPOwVcUJ/eWIWTSzHxd1s3pkoZR0jK2EYkP+RiDgLCEzfW9RT1owLFjBQ4ysAgotuAujCNAKKqmrGs0MLBhARrA4Wzr3eNzYF0xcPXDgGQ4ZlVt230eT8fw3LJuq9y26h/c2ghkIPnw8eUnA1ia3eIk6x798v8Lc2xhbHi+nKIYYE3xHidZSt5KO2tV2UU6CHinv4Ij9vpWnJomAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='athena-arch' title='athena-arch' src='/static/d46958f71cb7723842d6e83cd6b75861/8c557/athena-arch.png' srcset='/static/d46958f71cb7723842d6e83cd6b75861/4edbd/athena-arch.png 175w,\n/static/d46958f71cb7723842d6e83cd6b75861/13ae7/athena-arch.png 350w,\n/static/d46958f71cb7723842d6e83cd6b75861/8c557/athena-arch.png 700w,\n/static/d46958f71cb7723842d6e83cd6b75861/e996b/athena-arch.png 1050w,\n/static/d46958f71cb7723842d6e83cd6b75861/2cefc/athena-arch.png 1400w,\n/static/d46958f71cb7723842d6e83cd6b75861/2dc7d/athena-arch.png 1760w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span>\n对于类似于机器学习的下游任务来说，可通过Athena，从多种不同的入口获取序列化的数据。我们今天重点关注这些入口中的\"S3 data lake\"。</p>\n<p>为了完成这一工作，Athena需要提供两项基本的能力:</p>\n<ol>\n<li>提供一套统一的SQL接口，使得下游服务可以方便地获取数据，而不必担心底层的数据到底是如何储存的。</li>\n<li>解析SQL语句并转换为具体的任务，从底层数据存储中获取对应的数据并序列化</li>\n</ol>\n<p>为了解决这两个问题，Athena选择了<code class=\"language-text\">Trino/Presto</code>引擎。</p>\n<h2>2.1 Trino/Presto</h2>\n<p><code class=\"language-text\">Trino</code>是一个开源分布式的SQL查询引擎。如果要用一句话概括它的作用，那就是<code class=\"language-text\">SQL on Everything</code>。这也是Trino的前身<code class=\"language-text\">Presto</code>在发布时的论文标题: <a href=\"https://trino.io/Presto_SQL_on_Everything.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Presto: SQL on Everything</a></p>\n<p>Trino提供了<a href=\"https://blog.ansi.org/2018/10/sql-standard-iso-iec-9075-2016-ansi-x3-135/#gref\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ANSI SQL</a>接口并可以将其转换为下游任务，从而实现将SQL应用在各种各样的数据结构上。整个Presto的架构和流程如下图所示:\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 74.28571428571428%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAADmklEQVQ4y32Ta0xbZRjHa+IHbzHxk5lRd2EJ6JYYNy8kOOfiNGiCJBqDJvrduI/7sM3h/CBssg/OyMyCc0wnxDHAjctcC21puZbiuHQgg15p6QV6OT2nPadQ2v58KX5YvD0nvzzveU7Oc97/ef6vTlVVJEkiKctIsShSOETA5WTZ7SQS8BcJez2Efd6tvMmSD1XTSCtJ8moC1mQKGYn8uorO6/Vit9mYWnAy/NUpjC/sQH/gWQwvl2CsfhXTx+9geHEnhvLd9FWUFbP+9ecJeDwoiTCB27/htd9kaaIXbcUpGvp8TIyPM+1yM/LlCSzihcG3y7EeeAbzG/uwfFBZXA+8UsaguLdWlNIviC4vw0aasZtX6Pq5kb5rF1AjoqFfSJmcsDMzM830sJWpnk5G2lsZaGlmuv8WU0Z9sWbv7sTw48VintT3oCgphE7cw+30t5zDcauZjbgHXdtsnMNdEpW9KWrNAT6xJHnXtE5Vb5z3DQrVfRpHrVFODEY51C1TpU/znqivJFWxQ4UOk4XzN/po6ulHirjRNUwoPNq6weMd8ObVJfZ1KDzRDiW/Fii9nmebqFe0rVDZGWZ7F+zthe3tOQIJDbISn/3Sz0eXzHzYZCDkF5J9cY3rTpnW+RUcQQWzV6bLlabXlSpyw5nm91CaEWeEC9YFGo2zNFvnULQMQjPZ1CobyQDZ2CI5LYFOVJFXg8TDPv4rctkssXgCj7CT3+MU1nGSFPZS4zE0WSET8ZCJBckL6+jyhQJp8bW0qrG5/ntkN3LUXLbxVL2Vp88MUdMyicNwE/PBvZjEtI2bdtr/JKbD+1H8d9EV7mmSzeZosbn5zrrIOfMCo4tBWE9Q9f0ouxuGKD07RHXzOPbOqwxsWklgPbiH0df2MHToOZSl+S3JBXFl8jJBaZVd9Wa21Q3yQO0AR9rs4ukaK1KKwKpEICoRk2RSiiJOVBA5EiLi82Dq6WbMamVdWRUNxQZTmkxMDaGkVY5cu03NpVHeOm+hyTwjfmCSiyNujvfMcUzwg1j7HFPMnv2CuYbPcZw+yfCxTxmvO4kW9aPL5XJ4hbl9grXMWlH6sjirE7YxViNh0XCN8q8t3HfcyP2Cl74dZrT1MvqSR9DvfAjDjgcZ2PUwprLHkF2TW5LvjX/MRRR+GnNRZ5jndN9droh/7LszzR+NDcwLHN/UM3jqKLYztWLi0tZQivBX/hf+L3KCOx4fi8EQuXyBPwHWfN2MrJ0/JgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='presto-arch' title='presto-arch' src='/static/c94ff3c8a737a40142f4fa71cd89b1e9/8c557/presto-arch.png' srcset='/static/c94ff3c8a737a40142f4fa71cd89b1e9/4edbd/presto-arch.png 175w,\n/static/c94ff3c8a737a40142f4fa71cd89b1e9/13ae7/presto-arch.png 350w,\n/static/c94ff3c8a737a40142f4fa71cd89b1e9/8c557/presto-arch.png 700w,\n/static/c94ff3c8a737a40142f4fa71cd89b1e9/5b587/presto-arch.png 1010w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n    </span></p>\n<p>一个query从SQL到任务的整个过程可以被描述为:</p>\n<ul>\n<li>系统通过HTTP接收到SQL命令</li>\n<li>解析SQL并生成和优化分布式执行计划</li>\n<li>系统将计划分配给worker</li>\n<li>Worker接到任务后开始从外部储存读取数据</li>\n<li>根据任务图，并行执行任务并将结果传递到执行下一阶段任务的worker，最终最后一个worker将结果返回</li>\n</ul>\n<p>为了实现在各种不同的数据存储上进行SQL查询，Presto在整个流程中设计了几个Connector API接口。通过调用这些接口，Presto就会在任务整个生命周期中的各个环节从外部Connector获取必要信息，或是通过外部系统操作底层数据。这些接口是：</p>\n<ul>\n<li>Metadata API: 获取数据的metadata，其主要包括当前查询数据的Schema。</li>\n<li>Data Location API: 获取数据的位置，比如实现S3的Connector时，需要返回某张表在S3的具体地址。</li>\n<li>Data Source API: 从外部系统读取数据页。</li>\n<li>Data Sink API: 写数据到外部系统。</li>\n</ul>\n<p>可以看得出，Athena通过为不同的数据源实现了这几个API给Trino提供了一个Connector，就可以完成上述任务。</p>\n<p>说一些题外话：<code class=\"language-text\">Trino/Presto</code>一开始是2012年左右在Facebook内部开始的项目。Presto的创始团队是开源文化的拥趸，并且将Presto也打造成了一个开源项目，并和Airbnb, Dropbox, Netflix等公司的工程师们一起改善Presto。但对于Facebook来说打造完全开源的产品并维护一个良好的开源社区显然不是第一要务，相信大家从臭名昭著的<a href=\"https://medium.com/@raulk/if-youre-a-startup-you-should-not-use-react-reflecting-on-the-bsd-patents-license-b049d4a67dd2\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">React BSD+Patent license</a>事件多少可以看得出Facebook的态度。虽然React的lisence最后还是改成了MIT，但Presto就没那么好运了。2018年Facebook开始收紧对Presto的约束并要求把代码的写权限开放给毫无经验的Facebook工程师们。五位创始人别无选择，只能离开Facebook并把Presto更名为Trino继续用开源社区的方式进行维护。更详细的故事可以看看Trino的<a href=\"https://trino.io/blog/2022/08/02/leaving-facebook-meta-best-for-trino.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">这篇博客</a>。</p>\n<h1>3. AWS Glue</h1>\n<p>对于数据湖<code class=\"language-text\">Schema-on-read</code>的特性来说，如何维护这个\"Schema\"是一项关键的任务。向数据湖写数据的时候可以简单地将文件上传至S3，但经由Athena读取数据的时候必须指定数据的Schema，即向<code class=\"language-text\">Trino</code>的Metadata API提供对应数据的metadata。而<a href=\"https://aws.amazon.com/glue/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">AWS Glue</a>就承担了这一工作。</p>\n<p>AWS Glue有诸多功能，今天我们着重在其中两个最重要的部分：<code class=\"language-text\">Glue Data Catalog</code> 和 <code class=\"language-text\">Glue Crawler</code>。他们的关系如下图: <code class=\"language-text\">Glue Crawler</code>爬取数据湖中数据的Schema，并储存于<code class=\"language-text\">Glue Data Catalog</code>中。\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <a class='gatsby-resp-image-link' href='/static/1b5f2d9f68b67c866a994ab37b4a5ebb/e5715/glue-catalog-and-crawler.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 74.85714285714286%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACBUlEQVQoz5WT247TMBCG8948Ale8AhJIe4PESkiIQ1GL9qJ7gGWhkG2TpmmOTeIkduw4PuOkIAGCFWuNLPnwzdjzzzjGGNzTLE2aqhB8MPcZjqQ9TOPNxl/vkrLpUM+tNUQiKu2xvhvG4bZdLlZuOFsV81X+8sJ/dRV8ieDXGNpjpbSS6p8w6ocUtHGa+0Gw9rZhnN1uvPxwwISMkfVdsZ0csrim1oKiCwps7WNAvYJjOgZsS7g4ORsoG+9q+wxmND/OI9wzRQbZM82nP0ptsuYCgc8cnYPM3958e/t4hopyghkhONrtQJ4yRu3SaXtRdKZELsQvhDI2HMYLBq/R8KaAiAx2Q6AW1UQYia2H5dJ9/v5m9CWQc3y91lDKvdJGSMW5gFE+P/k0MMlstoQoyiSqay2pvUko9uN0JCR29E89jrkBTZtEIany0A8BKA9Vpbih6PXAZ9YTqlLJqRgIhZUWxPk1e5Zv2pZ0UPetXXb4GR5cLY3sR9k2u82j07NdvPf30ZN3l0UZ/QYLISzc1pUi7bTBb6/WRXgwwsqm3cB/eHoepsml6z14Ot/ufecv8ikpMairogHgevEhWfsTrHCTet6W4RrDKo5jRas/YX38OutFB0TXKI4MQ1pYVSXrDrCuBAEIZHG4ZV3m/HcXaCvsNMsfKdaDc58uUmNhTRU2Fpkx3wFN0lkAaN5F+QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='glue-catalog-and-crawler' title='glue-catalog-and-crawler' src='/static/1b5f2d9f68b67c866a994ab37b4a5ebb/8c557/glue-catalog-and-crawler.png' srcset='/static/1b5f2d9f68b67c866a994ab37b4a5ebb/4edbd/glue-catalog-and-crawler.png 175w,\n/static/1b5f2d9f68b67c866a994ab37b4a5ebb/13ae7/glue-catalog-and-crawler.png 350w,\n/static/1b5f2d9f68b67c866a994ab37b4a5ebb/8c557/glue-catalog-and-crawler.png 700w,\n/static/1b5f2d9f68b67c866a994ab37b4a5ebb/e5715/glue-catalog-and-crawler.png 768w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<h2>3.1 Glue Data Catalog</h2>\n<p>为了使用Athena查询数据，和普通的关系型数据库一样首先需要创建\"Database\"和\"Table\"，而\"Table\"本质就是对数据湖中数据规定的一系列Schema。最简单的创建方式是在Athena console中使用Trino SQL。假设我们有如下的账户数据(对于JSON格式的数据，Athena要求使用<a href=\"http://ndjson.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ndjson</a>格式，每一行是一条完整的json)</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span><span class=\"token property\">\"account_number\"</span><span class=\"token operator\">:</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"balance\"</span><span class=\"token operator\">:</span><span class=\"token number\">39225</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"firstname\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Amber\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"lastname\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Duke\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"age\"</span><span class=\"token operator\">:</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"gender\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"M\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"address\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"880 Holmes Lane\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"employer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Pyrami\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"email\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"amberduke@pyrami.com\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"city\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Brogan\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"state\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"IL\"</span><span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">{</span><span class=\"token property\">\"account_number\"</span><span class=\"token operator\">:</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"balance\"</span><span class=\"token operator\">:</span><span class=\"token number\">5686</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"firstname\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Hattie\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"lastname\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Bond\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"age\"</span><span class=\"token operator\">:</span><span class=\"token number\">36</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"gender\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"M\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"address\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"671 Bristol Street\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"employer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Netagy\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"email\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"hattiebond@netagy.com\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"city\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Dante\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"state\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"TN\"</span><span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">{</span><span class=\"token property\">\"account_number\"</span><span class=\"token operator\">:</span><span class=\"token number\">13</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"balance\"</span><span class=\"token operator\">:</span><span class=\"token number\">32838</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"firstname\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Nanette\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"lastname\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Bates\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"age\"</span><span class=\"token operator\">:</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"gender\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"F\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"address\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"789 Madison Street\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"employer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Quility\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"email\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"nanettebates@quility.com\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"city\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"Nogal\"</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"state\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"VA\"</span><span class=\"token punctuation\">}</span></code></pre></div>\n<p>该数据储存在<code class=\"language-text\">athena-data</code>bucket的<code class=\"language-text\">accounts</code>文件夹下，则运行SQL</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> EXTERNAL <span class=\"token keyword\">TABLE</span> <span class=\"token punctuation\">`</span>accounts<span class=\"token punctuation\">`</span><span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">`</span>account_number<span class=\"token punctuation\">`</span> <span class=\"token keyword\">int</span><span class=\"token punctuation\">,</span> \n  <span class=\"token punctuation\">`</span>balance<span class=\"token punctuation\">`</span> <span class=\"token keyword\">bigint</span><span class=\"token punctuation\">,</span> \n  <span class=\"token punctuation\">`</span>firstname<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>lastname<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>age<span class=\"token punctuation\">`</span> <span class=\"token keyword\">int</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>gender<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>address<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>employer<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>email<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>city<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>state<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">ROW</span> FORMAT SERDE \n  <span class=\"token string\">'org.openx.data.jsonserde.JsonSerDe'</span> <span class=\"token comment\">-- 指定SerDe</span>\nLOCATION\n  <span class=\"token string\">'s3://athena-data/accounts'</span> <span class=\"token comment\">-- 指定S3地址</span></code></pre></div>\n<p>这样，一张Glue Data Catalog的就创建完成了。注意与在<code class=\"language-text\">MySQL</code>中创建表不同的是，这里需要指定<code class=\"language-text\">ROW FORMAT</code>和S3<code class=\"language-text\">LOCATION</code>。其中<code class=\"language-text\">org.openx.data.jsonserde.JsonSerDe</code>代表使用内置的JSON序列化/反序列化去处理S3中的数据，如果源数据的格式不是JSON，而是诸如<code class=\"language-text\">csv</code>或<code class=\"language-text\">Parquet</code>之类的格式，则要指定对应的SerDe。</p>\n<p>创建完成后，就可以使用Trino SQL在Athena中查询数据了:</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> <span class=\"token string\">\"accounts\"</span> <span class=\"token keyword\">limit</span> <span class=\"token number\">10</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>每次查询的结果都会保存为csv格式存放在指定的S3目录下。</p>\n<h2>3.1 Glue Crawler</h2>\n<p>手动使用SQL创建表很方便，但是仍然有很多问题:</p>\n<ul>\n<li>如果底层json数据的schema有变化，例如新增字段了，需要手动维护表结构</li>\n<li>数据查询时会扫描所有的json文件，因为Schema-on-read并不像传统SQL一样会建立index。数据插入时是没有Schema的，所以不具备动态维护index的条件</li>\n</ul>\n<p>为了解决这些问题，我们可以使用Glue Crawler来自动爬取S3中数据的Schema，并为数据创建partition。</p>\n<p>首先，我们为Crawler指定一个目标S3路径，比如</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">s3://athena-data/accounts/</code></pre></div>\n<p>以及输出的database，那么Crawler就会扫描<code class=\"language-text\">s3://athena-data/accounts/</code>下的文件(可以配置成仅扫描上次扫描以来的新增文件)，并在目标database中创建一张名为\"accounts\"的表(与S3目标地址最后一级目录名相同)。在扫描的过程中，crawler会自动按照文件的格式选取合适的SerDe进行序列化/反序列化，并提取文件中的字段形成对应的Schema储存在表结构中。</p>\n<p>其次，为了提高查询时的效率，我们需要为table创建partition。Partition是通过文件夹结构创建在数据某些字段上的。例如上述的<code class=\"language-text\">accounts</code>数据，如果现在想按照\"state\"和\"city\"字段创建partition，使得我们在查询某个\"city\"数据时不会去扫描别的city目录下的数据，我们需要将同一个state和city的数据组织在同一个目录下，例如:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">s3://athena-data/accounts/state=va/city=nogal/accounts.json\ns3://athena-data/accounts/state=tn/city=dante/accounts.json</code></pre></div>\n<p>使用这种形如<code class=\"language-text\">key=value</code>格式的文件夹名，那么crawler在爬取数据的时候就会自动在data catalog中加入对应的partition。当我们运行SQL</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> <span class=\"token string\">\"accounts\"</span> <span class=\"token keyword\">where</span> city<span class=\"token operator\">=</span><span class=\"token string\">'nogal'</span> <span class=\"token keyword\">limit</span> <span class=\"token number\">10</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>加入其他state文件夹中不存在名为'city=nogal'的文件夹了，那么这次查询Athena只会扫描<code class=\"language-text\">s3://athena-data/accounts/state=va/city=nogal/</code>下的文件。</p>\n<p>此时我们查看表<code class=\"language-text\">accounts</code>的创建DDL</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SHOW</span> <span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> <span class=\"token punctuation\">`</span>accounts<span class=\"token punctuation\">`</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>可以得到如下结果</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> EXTERNAL <span class=\"token keyword\">TABLE</span> <span class=\"token punctuation\">`</span>accounts<span class=\"token punctuation\">`</span><span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">`</span>account_number<span class=\"token punctuation\">`</span> <span class=\"token keyword\">int</span>  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token punctuation\">`</span>balance<span class=\"token punctuation\">`</span> <span class=\"token keyword\">bigint</span>  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token punctuation\">`</span>firstname<span class=\"token punctuation\">`</span> string  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>lastname<span class=\"token punctuation\">`</span> string  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>age<span class=\"token punctuation\">`</span> <span class=\"token keyword\">int</span>  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>gender<span class=\"token punctuation\">`</span> string  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>address<span class=\"token punctuation\">`</span> string  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>employer<span class=\"token punctuation\">`</span> string  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>email<span class=\"token punctuation\">`</span> string  <span class=\"token keyword\">COMMENT</span> <span class=\"token string\">'from deserializer'</span><span class=\"token punctuation\">)</span>\nPARTITIONED <span class=\"token keyword\">BY</span> <span class=\"token punctuation\">(</span> \n  <span class=\"token punctuation\">`</span>city<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">`</span>state<span class=\"token punctuation\">`</span> string<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">ROW</span> FORMAT SERDE \n  <span class=\"token string\">'org.openx.data.jsonserde.JsonSerDe'</span> \n<span class=\"token keyword\">WITH</span> SERDEPROPERTIES <span class=\"token punctuation\">(</span> \n  <span class=\"token string\">'paths'</span><span class=\"token operator\">=</span><span class=\"token string\">'account_number,balance,firstname,lastname,age,gender,address,employer'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'email'</span><span class=\"token punctuation\">)</span> \nSTORED <span class=\"token keyword\">AS</span> INPUTFORMAT \n  <span class=\"token string\">'org.apache.hadoop.mapred.TextInputFormat'</span> \nOUTPUTFORMAT \n  <span class=\"token string\">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span>\nLOCATION\n  <span class=\"token string\">'s3://athena-data/accounts'</span>\nTBLPROPERTIES <span class=\"token punctuation\">(</span>\n  <span class=\"token string\">'CrawlerSchemaDeserializerVersion'</span><span class=\"token operator\">=</span><span class=\"token string\">'1.0'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'CrawlerSchemaSerializerVersion'</span><span class=\"token operator\">=</span><span class=\"token string\">'1.0'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'UPDATED_BY_CRAWLER'</span><span class=\"token operator\">=</span><span class=\"token string\">'accounts-crawler'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'averageRecordSize'</span><span class=\"token operator\">=</span><span class=\"token string\">'374'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'classification'</span><span class=\"token operator\">=</span><span class=\"token string\">'json'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'compressionType'</span><span class=\"token operator\">=</span><span class=\"token string\">'none'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'objectCount'</span><span class=\"token operator\">=</span><span class=\"token string\">'1309'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'recordCount'</span><span class=\"token operator\">=</span><span class=\"token string\">'1309'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'sizeKey'</span><span class=\"token operator\">=</span><span class=\"token string\">'630830'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'typeOfData'</span><span class=\"token operator\">=</span><span class=\"token string\">'file'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>可以看到除了一些crawler的元信息外，主要多了<code class=\"language-text\">PARTITIONED BY</code>字段。并且将来有更多数据加入数据湖时，定期使用Crawler爬取数据可以自动更新表结构和Partition。需要注意的是如果有新的partition文件夹产生了，那么这些新partition中的数据在被Crawler爬取或手动增加partition之前，是无法被查询到的。</p>\n<h1>4. 总结</h1>\n<p>这次我们从总体上了解了构成AWS数据湖的几个最基本的服务：AWS S3, Athena和Glue，并介绍了他们各自在整个数据湖架构中的位置和作用。在下篇文章中，我们详细了解如何搭建这一套解决方案，并通过解决一些实际的问题:</p>\n<ul>\n<li>如果每一条数据都是一个ndjson文件，那么就算使用了partition，一个partition内的文件数过多是否会影响查询性能？</li>\n<li>通过Athena查询时使用了不在partition上的<code class=\"language-text\">where</code>条件，有没有办法有效地降低文件扫描的数量和耗时？</li>\n</ul>\n<h1>参考文献</h1>\n<ol>\n<li><a href=\"https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/</a></li>\n<li><a href=\"https://aws.amazon.com/redshift/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://aws.amazon.com/redshift/</a></li>\n<li><a href=\"https://trino.io/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://trino.io/</a></li>\n<li><a href=\"https://spark.apache.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://spark.apache.org/</a></li>\n<li><a href=\"https://trino.io/Presto_SQL_on_Everything.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://trino.io/Presto_SQL_on_Everything.pdf</a></li>\n<li><a href=\"https://aws.amazon.com/glue/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://aws.amazon.com/glue/</a></li>\n<li><a href=\"http://ndjson.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">http://ndjson.org/</a></li>\n</ol>","excerpt":"机器学习和大数据分析技术在过去十年内高速发展，到今天已经成为了现代化的服务与应用不可或缺的一部分。而搭建一套数据湖系统则是开始将机器学习与数据分析带到项目中的第一步。在这几篇文章里，我们将会学习到如何使用服务与文件存储搭建一套完整的数据湖解决方案。 1. AWS的数据湖 在AWS的这篇文章中，阐述了与的主要区别。简单地概括一下即： “数据仓库”一般指存储结构化业务数据的系统，数据的Schema在数据存储时就已经确定()，这种系统经常用来做批量的业务数据分析以及可视化。比较典型的例子就是AWS…","frontmatter":{"date":"December 21, 2022","path":"/aws-athena-s3-data-lake-architecture","title":"使用AWS Athena搭建基于S3的数据湖(1): 概念与架构"}}},"pageContext":{}},"staticQueryHashes":["1176552510","3649515864","63159454","846684790"]}