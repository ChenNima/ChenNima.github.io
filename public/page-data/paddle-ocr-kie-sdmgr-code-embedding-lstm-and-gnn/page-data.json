{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/paddle-ocr-kie-sdmgr-code-embedding-lstm-and-gnn","result":{"data":{"markdownRemark":{"html":"<p>在上篇文章<a href=\"/paddle-ocr-kie-sdmgr-code-data-process-and-backbone\">关键信息提取网络SDMGR代码详解(2): 数据处理与主干网络</a>中我们结合代码介绍了<a href=\"https://github.com/PaddlePaddle/PaddleOCR\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">PaddleOCR</a>中用于关键信息提取(KIE)任务网络<a href=\"https://arxiv.org/abs/2103.14470v1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SDMGR</a>(Spatial Dual-Modality Graph Reasoning for Key Information Extraction)的主干网络部分。今天我们了解对文字部分处理的循环神经网络LSTM与融合多模态特征后进行图推理的图神经网络模块。而这两个模块也是SDMGR网络中最重要也是最复杂的模块。</p>\n<p><span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <a class='gatsby-resp-image-link' href='/static/9f2841aa6e29db8849739d5cc5f853a4/56873/sdmgr-net.jpg' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 46.28571428571429%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAEDAgX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHt6mygg//EABcQAAMBAAAAAAAAAAAAAAAAAAABEEH/2gAIAQEAAQUC0VR//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhAAAwAAAAAAAAAAAAAAAAAAACAx/9oACAEBAAY/Air/AP/EABwQAAEEAwEAAAAAAAAAAAAAAAABESExEEFhof/aAAgBAQABPyFpT6PwUlXNrip//9oADAMBAAIAAwAAABBwD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB0QAAICAQUAAAAAAAAAAAAAAAERACExEGFxofH/2gAIAQEAAT8QQEJ2dijeEdkWyIO/TLzP/9k='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='sdmgr-net' title='sdmgr-net' src='/static/9f2841aa6e29db8849739d5cc5f853a4/29d31/sdmgr-net.jpg' srcset='/static/9f2841aa6e29db8849739d5cc5f853a4/e52aa/sdmgr-net.jpg 175w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/70ebb/sdmgr-net.jpg 350w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/29d31/sdmgr-net.jpg 700w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/9ecec/sdmgr-net.jpg 1050w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/d165a/sdmgr-net.jpg 1400w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/56873/sdmgr-net.jpg 2332w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<p>这两部分的代码都位于<code class=\"language-text\">ppocr/modeling/heads/kie_sdmgr_head.py</code>。</p>\n<h1>1. 文字嵌入(Embedding)</h1>\n<p>在经过数据预处理和主干网络后，批量中每一条数据的文字部分都被处理成了一个[num_nodes, recoder_len]大小的矩阵。其中<code class=\"language-text\">num_nodes</code>代表了这条数据中一共有多少个文字节点，而<code class=\"language-text\">recoder_len</code>则表示了这条数据的所有文字记录中，最长的那条文字的长度。对于长度不足<code class=\"language-text\">recoder_len</code>的文字，则用<code class=\"language-text\">-1</code>补齐到<code class=\"language-text\">recoder_len</code>的长度。现在每条文字对应的向量中，除了padding所用的-1外，其余的元素均使用字典将文字转换到了<code class=\"language-text\">float32</code>格式的数字标号。而为了能使得LSTM能够处理这些文字信息，仍然需要将这些<code class=\"language-text\">float32</code>格式的数字标号转换为长度固定的向量表示，也就是说需要完成文字的嵌入。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">relations<span class=\"token punctuation\">,</span> texts<span class=\"token punctuation\">,</span> x <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span>\nnode_nums<span class=\"token punctuation\">,</span> char_nums <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> text <span class=\"token keyword\">in</span> texts<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># text.shape[0]获取了一条数据中文字节点的数量</span>\n    <span class=\"token comment\"># node_nums则记录了一个batch中所有数据的文字节点数量，形状为[batch_size]</span>\n    node_nums<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># 计算每条文字去除padding后的长度，形成形状为[num_nodes]的向量</span>\n    <span class=\"token comment\"># char_nums形状为[batch_size, num_nodes]，记录了每个batch各个文字的真实长度</span>\n    char_nums<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>paddle<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>text <span class=\"token operator\">></span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># max_num计算出了当前所有batch中最长的文字长度, shape为[1]。仅包含一个元素</span>\nmax_num <span class=\"token operator\">=</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>char_num<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> char_num <span class=\"token keyword\">in</span> char_nums<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 首先看内层，将每个批量中text([num_nodes, recoder_len])与一个全0矩阵([num_nodes, max_num - recoder_len])在最后一个维度拼接</span>\n<span class=\"token comment\"># 其实就是用全0矩阵将text pad成[num_nodes, max_num]的形状，使得整个批量的所有文字在长度维都固定为max_num</span>\n<span class=\"token comment\"># 最终将批量内所有的文字在第一维也就是num_nodes维度拼接</span>\n<span class=\"token comment\"># 形成all_nodes，形状为[all_num_nodes, max_num]</span>\nall_nodes <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>\n        <span class=\"token punctuation\">[</span>text<span class=\"token punctuation\">,</span> paddle<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>\n            <span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> max_num <span class=\"token operator\">-</span> text<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> text <span class=\"token keyword\">in</span> texts\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 最小值限定为0，将残余的-1 padding去除</span>\ntemp <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>clip<span class=\"token punctuation\">(</span>all_nodes<span class=\"token punctuation\">,</span> <span class=\"token builtin\">min</span><span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 进行批量整体的词嵌入</span>\nembed_nodes <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>node_embed<span class=\"token punctuation\">(</span>temp<span class=\"token punctuation\">)</span></code></pre></div>\n<p>从代码中我们可以得知，在真正的Embedding之前，需要将当前批量中所有的文字信息合并到一起，形成一个形状为<code class=\"language-text\">[all_num_nodes, max_num]</code>的矩阵，其中<code class=\"language-text\">all_num_nodes</code>是当前批量中所有文字信息的总数，<code class=\"language-text\">max_num</code>是这些文字中最长的长度。那些较短的文字与<code class=\"language-text\">max_num</code>之间的差距都用<code class=\"language-text\">0</code>做Padding。</p>\n<p>其中嵌入层的定义为</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">self<span class=\"token punctuation\">.</span>node_embed <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>num_chars<span class=\"token punctuation\">,</span> node_input<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">num_chars</code>为字典的长度，而<code class=\"language-text\">node_input</code>是一个超参数(默认为32)，即每个词嵌入后的向量长度。第三个参数<code class=\"language-text\">padding_idx=0</code>表示所有的<code class=\"language-text\">0</code>都只是用作Padding的。\n经过Embedding后，每个字符都从单个数字被展开为了长度为<code class=\"language-text\">node_input</code>的向量。最终输入到LSTM中的文字矩阵形状为<code class=\"language-text\">[all_num_nodes, max_num, node_input]</code>。</p>\n<h1>2. 循环神经网络LSTM</h1>\n<p><a href=\"https://en.wikipedia.org/wiki/Long_short-term_memory\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">LSTM</a>(Long short-term memory)即长短期记忆是在RNN的基础上增加了一个新的传输状态<code class=\"language-text\">c(t)</code>(cell state)，与原有的传输状态<code class=\"language-text\">h(t)</code>共同组成了长期和短期的记忆。关于LSTM详细的技术细节这里不再赘述，对于每一个时间步<code class=\"language-text\">t</code>的输入<code class=\"language-text\">x(t)</code>，LSTM输出一个隐变量<code class=\"language-text\">h(t)</code>。SDMGR也是利用这个机制，把LSTM当作了对文字信息的编码器使用。</p>\n<p><img src=\"/static/LSTM_Cell-f765798282de5ebf587e2ed1f514359e.svg\" alt=\"LSTM_cell\"></p>\n<p>我们先来看LSTM的初始化</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">hidden <span class=\"token operator\">=</span> node_embed <span class=\"token operator\">//</span> <span class=\"token number\">2</span> <span class=\"token keyword\">if</span> bidirectional <span class=\"token keyword\">else</span> node_embed\nself<span class=\"token punctuation\">.</span>rnn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>\n    input_size<span class=\"token operator\">=</span>node_input<span class=\"token punctuation\">,</span> hidden_size<span class=\"token operator\">=</span>hidden<span class=\"token punctuation\">,</span> num_layers<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>这里直接使用了Paddle对LSTM的实现，传入的参数分别为</p>\n<ul>\n<li>input_size=node_input: 决定了输入<code class=\"language-text\">x(t)</code>的形状。<code class=\"language-text\">node_input</code>作为超参数决定了词嵌入后每个字符对应的向量长度，也决定了LSTM的输入形状。</li>\n<li>hidden_size=hidden：<code class=\"language-text\">node_embed</code>也是一个超参数，决定了LSTM隐藏层的大小以及输出向量的长度。这里<code class=\"language-text\">node_embed</code>默认为256，如果模型选择了双向<code class=\"language-text\">bidirectional=true</code>则隐藏层大小会减半，但实际并未将LSTM变为双向LSTM。</li>\n<li>num_layers=1：LSTM的层数固定为1</li>\n</ul>\n<p>将上一步嵌入完成的文字输入LSTM网络即可得到编码完成的文字特征<code class=\"language-text\">rnn_nodes</code>，形状为[all_num_nodes, max_num, hidden_size]。即将上一步得到的<code class=\"language-text\">embed_nodes</code>的最后一个维度从<code class=\"language-text\">node_input</code>变为了<code class=\"language-text\">hidden_size</code>, <code class=\"language-text\">hidden_size</code>默认为256。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">rnn_nodes<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>rnn<span class=\"token punctuation\">(</span>embed_nodes<span class=\"token punctuation\">)</span></code></pre></div>\n<p>LSTM输出的结果包含了各个node文字序列中，每一个时间步的输出。但对于SDMGR来说，仅仅需要获得整句句子总体的特征。所以需要把这个结果中除了最后一个元素的隐状态输出外的其他结果全部去掉。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># b: all_num_nodes</span>\n<span class=\"token comment\"># h: max_num</span>\n<span class=\"token comment\"># w: hidden_size</span>\nb<span class=\"token punctuation\">,</span> h<span class=\"token punctuation\">,</span> w <span class=\"token operator\">=</span> rnn_nodes<span class=\"token punctuation\">.</span>shape\n<span class=\"token comment\"># 所有的node，形状为[all_num_nodes, hidden_size]</span>\nnodes <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>b<span class=\"token punctuation\">,</span> w<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 一维向量，批量中所有文字的长度，该向量长度为批量中所有节点的个数，即all_num_nodes</span>\nall_nums <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>char_nums<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 获取长度不为0的文字index，shape为[all_num_nodes, 1]</span>\nvalid <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>nonzero<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>all_nums <span class=\"token operator\">></span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># (paddle.gather(all_nums, valid) - 1) 先将所有不为0的长度挑选出来，并且长度 -1</span>\n<span class=\"token comment\"># 经过两次unsqueeze，temp_all_nums的形状变为了[all_num_nodes, 1, 1]</span>\ntemp_all_nums <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>\n    paddle<span class=\"token punctuation\">.</span>gather<span class=\"token punctuation\">(</span>all_nums<span class=\"token punctuation\">,</span> valid<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># temp_all_nums形状变为了[all_num_nodes, 1, hidden_size]， hidden_size=256，最后一维即文字长度被复制了256次</span>\ntemp_all_nums <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>expand<span class=\"token punctuation\">(</span>temp_all_nums<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>\n    temp_all_nums<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> temp_all_nums<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> rnn_nodes<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># temp_all_nodes的形状不变，维持与rnn_nodes相同，剔除了文字长度为1的节点</span>\ntemp_all_nodes <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>gather<span class=\"token punctuation\">(</span>rnn_nodes<span class=\"token punctuation\">,</span> valid<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># N: all_num_nodes - 批量中所有节点个数</span>\n<span class=\"token comment\"># C: max_num - 所有文字中最长的长度</span>\n<span class=\"token comment\"># A: hidden_size - RNN隐藏层大小，默认256</span>\nN<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">,</span> A <span class=\"token operator\">=</span> temp_all_nodes<span class=\"token punctuation\">.</span>shape\n<span class=\"token comment\"># one_hot为对各node按照C=max_num进行独热编码</span>\n<span class=\"token comment\"># 即按照文字的长度进行分类</span>\n<span class=\"token comment\"># 独热编码后交换第二第三维顺序，整体shape变为与temp_all_nodes相同的[all_num_nodes, max_num, hidden_size]</span>\n<span class=\"token comment\"># 第二维大小max_num即独热编码后的向量, 第三维hidden_size为全0或者全1</span>\none_hot <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>one_hot<span class=\"token punctuation\">(</span>\n    temp_all_nums<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> num_classes<span class=\"token operator\">=</span>C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 将RNN的输出temp_all_nodes与形状相同的独热编码one_hot按元素相乘</span>\n<span class=\"token comment\"># 因为one_hot的第三维为全0或者全1，RNN每个元素的输出multiply后仅保留了序列中最后一个元素的输出</span>\n<span class=\"token comment\"># 在第一维求和后，因为除了最后一个元素输出其他全为0，所以得到形状为[all_num_nodes, 1, hidden_size]</span>\n<span class=\"token comment\"># 每个node对应文字序列的最后一个字的RNN输出</span>\none_hot <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>multiply<span class=\"token punctuation\">(</span>\n    temp_all_nodes<span class=\"token punctuation\">,</span> one_hot<span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token string\">\"float32\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdim<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 去掉中间，RNN输出shape变为[all_num_nodes, hidden_size]</span>\nt <span class=\"token operator\">=</span> one_hot<span class=\"token punctuation\">.</span>expand<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>N<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> A<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>squeeze<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># valid.squeeze(1)是一个shape为[all_num_nodes]的mask，用于过滤长度为0的node</span>\n<span class=\"token comment\"># 最终，RNN的隐藏层保留了序列中最后一个元素的输出，并且过滤掉了成都为0的node</span>\nnodes <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>nodes<span class=\"token punctuation\">,</span> valid<span class=\"token punctuation\">.</span>squeeze<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">)</span></code></pre></div>\n<p>整个流程的计算相对比较复杂，结合代码我们可以知道从长短不一的RNN输出中挑选出最后一个元素的输出，这里巧妙地使用了独热编码：</p>\n<blockquote>\n<p>例如假设全体文字最长地长度<code class=\"language-text\">max_num</code>为3，那么1, 2, 3就会被分别编码为向量[1, 0, 0], [0, 1, 0] 和 [0, 0, 1]。\n不用考虑0是因为长度为0的节点在这是已经被剔除了。\n那么实际长度为2的序列[11, 12]由于被padding到了长度max_num=3，就成为了[11, 12, 0]。\n此时想要获得实际长度的最后一个元素12，那就可以先对长度2进行num_classes=3的独热编码获得[0, 1, 0]，然后与向量[11, 12, 0]按元素乘积再累加，就可以获得元素12了。</p>\n</blockquote>\n<p>看起来相当繁琐，但对于比较大的矩阵来说，这样的操作可以通过数次矩阵运算，轻松地计算出所有节点地输出。最终LSTM的输出形状为<code class=\"language-text\">[all_num_nodes, hidden_size]</code>。</p>\n<h1>3. 特征融合</h1>\n<p>到现在为止，我们以及得到了每个文字对应的图片特征以及每个文字的特征，接下来我们就可以将文字特征与图片特征融合在一起了。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">self<span class=\"token punctuation\">.</span>fusion <span class=\"token operator\">=</span> Block<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>visual_dim<span class=\"token punctuation\">,</span> node_embed<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> node_embed<span class=\"token punctuation\">,</span> fusion_dim<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># x即经过ROI后的图片特征图</span>\n<span class=\"token comment\"># x的shape为[all_num_nodes, 16]，即所有节点对应的16维通道特征</span>\n<span class=\"token comment\"># 输出的shape与文字节点shape保持一致，为[all_num_nodes, hidden_size]</span>\n<span class=\"token keyword\">if</span> x <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n    nodes <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fusion<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">,</span> nodes<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>从调用就能看出这里将图片信息x与文字信息nodes融合。如果不存在图片，模型也可以单纯使用文字。</p>\n<p>SDMGR使用了<a href=\"https://zh.m.wikipedia.org/zh/%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%E7%A7%AF\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">克罗内克积</a>来融合两个特征。克罗内克积是外积从向量到矩阵的推广：</p>\n<p><img src=\"/static/kronecker-6716e37c4f62377a6c504565ee9ca62c.svg\" alt=\"kronecker\"></p>\n<p>可以看到克罗内克积的结果是一个分块矩阵，[m, a]矩阵与[n, b]矩阵德克罗内克积形状是[m*n, a*b]。直白地理解就是左侧矩阵中的每个元素与右侧矩阵相乘，获得的结果形状与右侧矩阵相同，放置于结果中左侧矩阵的相对位置。举个例子：</p>\n<p><img src=\"/static/kronecker_example-0ebf2e933f0f01a28a0698d3ff76a672.svg\" alt=\"kronecker_example\"></p>\n<p>SDMGR使用的克罗内克积基于计算复杂度的考虑进行了修改。输入x与nodes会分别通过一个全连接层统一变成shape为[all_num_nodes, 1024]的矩阵。经过一系列修改版的克罗内克积的操作后仍然保持了[all_num_nodes, 1024]，最后通过一个全连接层转换为[all_num_nodes, hidden_size]的矩阵输出。</p>\n<p>克罗内克积在物理中经常使用，但在机器学习领域不常出现。SDMGR的改版克罗内克积详细过程这里不在赘述。</p>\n<h1>4. 图神经网络GNN</h1>\n<p>SDMGR的一大亮点就是使用图神经网络来处理节点与空间信息。图神经网络作为近年来新出现的神经网络类型，尚处在发展阶段，在工业界的应用也远不如卷积，循环和最近大热的Transformer架构这般广泛。对图神经网络感兴趣的话可以看看Distill的这篇互动式博客<a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">A Gentle Introduction to Graph Neural Networks</a>非常直观地解释了什么是图神经网络。李沐也基于这篇博客进行了讲解<a href=\"https://www.bilibili.com/video/BV1iT4y1d7zP\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">零基础多图详解图神经网络（GNN/GCN）</a>。接下来我们一起来看看SDMGR是怎么构造图神经网络的：</p>\n<p>在上面的特征融合后我们得到了构成图神经网络中节点的一部分，接下来就要开始基于空间信息构造边了。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># relations的shape为[batch_size, num_nodes, num_nodes, 5], 记录了每条数据中边缘框与边缘框之间的关系</span>\n<span class=\"token comment\"># 将batch中所有的relation拼接起来后，形成了all_edges, shape为[num_nodes*num_nodes*n, 5]</span>\nall_edges <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>\n    <span class=\"token punctuation\">[</span>rel<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> rel<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> rel <span class=\"token keyword\">in</span> relations<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># edge_embed是一个全连接层，将5维的空间信息扩展为256维</span>\nembed_edges <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>edge_embed<span class=\"token punctuation\">(</span>all_edges<span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token string\">'float32'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 对embed_edges做L2 norm</span>\nembed_edges <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>normalize<span class=\"token punctuation\">(</span>embed_edges<span class=\"token punctuation\">)</span></code></pre></div>\n<p>经过一个全连接和L2范数归一化后，我们获得了空间信息特征，形状为[num_nodes*num_nodes*n, 256]，就可以进行图神经网络迭代了</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> gnn_layer <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>gnn_layers<span class=\"token punctuation\">:</span>\n    nodes<span class=\"token punctuation\">,</span> cat_nodes <span class=\"token operator\">=</span> gnn_layer<span class=\"token punctuation\">(</span>nodes<span class=\"token punctuation\">,</span> embed_edges<span class=\"token punctuation\">,</span> node_nums<span class=\"token punctuation\">)</span></code></pre></div>\n<p>重新回顾一下各个参数：</p>\n<ul>\n<li>nodes：文字与图片信息的融合，shape=[all_num_nodes, hidden_size]</li>\n<li>embed_edges：空间信息，shape=[num_nodes*num_nodes*n, 256]</li>\n<li>node_nums：batch中每条数据的节点数，shape=[batch_size]</li>\n</ul>\n<p>默认该迭代会重复两次，不断更新输出<code class=\"language-text\">nodes</code>与<code class=\"language-text\">cat_nodes</code>，其中<code class=\"language-text\">nodes</code>会作为新的节点特征进入下一次迭代</p>\n<p>我们来看一下图神经网络是如何定义前向计算的:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> nodes<span class=\"token punctuation\">,</span> edges<span class=\"token punctuation\">,</span> nums<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    start<span class=\"token punctuation\">,</span> cat_nodes <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> num <span class=\"token keyword\">in</span> nums<span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># 根据每条数据的节点数，从batch中截取当前数据的所有节点</span>\n        <span class=\"token comment\"># sample_nodes shape = [num_nodes, hidden_size]</span>\n        sample_nodes <span class=\"token operator\">=</span> nodes<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">:</span>start <span class=\"token operator\">+</span> num<span class=\"token punctuation\">]</span>\n        <span class=\"token comment\"># sample_nodes.unsqueeze(1) shape = [num_nodes, 1, hidden_size]</span>\n        <span class=\"token comment\"># sample_nodes.unsqueeze(0) shape = [1, num_nodes, hidden_size]</span>\n        <span class=\"token comment\"># 经过expand后这两个变量的shape都变为了[num_nodes, num_nodes, hidden_size]</span>\n        <span class=\"token comment\"># 虽然形状一样但是内容有所区别，一个是在第0维度上从1复制为num_nodes，另一个是在第1维度上复制。</span>\n        <span class=\"token comment\"># 这两种不同的复制方式其实分别代表了节点i对应其他节点的关系，以及其他节点对用节点i的关系</span>\n        <span class=\"token comment\"># reshape后最终的cat_nodes的shepe为[batch_size, num_nodes*num_nodes, hidden_size*2]</span>\n        cat_nodes<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>\n            paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n                paddle<span class=\"token punctuation\">.</span>expand<span class=\"token punctuation\">(</span>sample_nodes<span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> num<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                paddle<span class=\"token punctuation\">.</span>expand<span class=\"token punctuation\">(</span>sample_nodes<span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>num<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>num<span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        start <span class=\"token operator\">+=</span> num\n    <span class=\"token comment\"># paddle.concat(cat_nodes)后shape为[num_nodes*num_nodes*n, hidden_size*2]，hidden_size默认为256</span>\n    <span class=\"token comment\"># cat_nodes concat了edges后，最终的shape为[num_nodes*num_nodes*n, 768]</span>\n    cat_nodes <span class=\"token operator\">=</span> paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>cat_nodes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> edges<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># in_fc是一个全连接，将feature维度从node_dim * 2 + edge_dim=768降低到node_dim=256</span>\n    <span class=\"token comment\"># 此时cat_nodes shape=[num_nodes*num_nodes*n, 256]</span>\n    <span class=\"token comment\"># cat_nodes代表了节点的特征与两节点间的空间关系</span>\n    cat_nodes <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>in_fc<span class=\"token punctuation\">(</span>cat_nodes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># coef_fc是一个全连接，将256维的feature降为1维</span>\n    <span class=\"token comment\"># coefs shape=[num_nodes*num_nodes*n, 1]</span>\n    <span class=\"token comment\"># 而coefs经过归一化后代表了可学习的两个节点间边的权重</span>\n    coefs <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>coef_fc<span class=\"token punctuation\">(</span>cat_nodes<span class=\"token punctuation\">)</span>\n\n    start<span class=\"token punctuation\">,</span> residuals <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> num <span class=\"token keyword\">in</span> nums<span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># 将coefs变换成[num_nodes, num_nodes, 1]的张量，然后每一行做softmax归一化</span>\n        <span class=\"token comment\"># 这里为了让每个节点自己与自己的关系降为0，不影响归一化的结果，在softmax之前在对角线上的元素减去了1e9。</span>\n        <span class=\"token comment\"># residual即学习的两个节点间边的权重</span>\n        <span class=\"token comment\"># residual的shape为[num_nodes, num_nodes, 1]</span>\n        residual <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>\n            <span class=\"token operator\">-</span>paddle<span class=\"token punctuation\">.</span>eye<span class=\"token punctuation\">(</span>num<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">1e9</span> <span class=\"token operator\">+</span>\n            coefs<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">:</span>start <span class=\"token operator\">+</span> num<span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>num<span class=\"token punctuation\">,</span> num<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># 将边权重与空间特征相乘，获得了每条数据整体的特征，shape为[num_nodes, num_nodes, 256]</span>\n        <span class=\"token comment\"># 在维度1上加和特征，由于系数residual经过归一化，相当于对节点i，计算了与其他节点之间关系特征的加权平均数</span>\n        <span class=\"token comment\"># 最终特征的shape为[num_nodes, 256]</span>\n        residuals<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>residual <span class=\"token operator\">*</span> cat_nodes<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">:</span>start <span class=\"token operator\">+</span> num<span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span>\n                            <span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>num<span class=\"token punctuation\">,</span> num<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        start <span class=\"token operator\">+=</span> num<span class=\"token operator\">**</span><span class=\"token number\">2</span>\n\n    <span class=\"token comment\"># 将上述计算获得的特征拼接后经过一层输入和输出维度相等的全连接层，再与nodes相加</span>\n    <span class=\"token comment\"># 相加也说明杉树所学习到的整体特征实际是一个残差值residual</span>\n    <span class=\"token comment\"># 最终输出nodes shape=[num_nodes, 256]，代表节点特征</span>\n    <span class=\"token comment\"># cat_nodes shape=[num_nodes*num_nodes*n, 256]，代表边特征</span>\n    nodes <span class=\"token operator\">+=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>out_fc<span class=\"token punctuation\">(</span>paddle<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>residuals<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>nodes<span class=\"token punctuation\">,</span> cat_nodes<span class=\"token punctuation\">]</span></code></pre></div>\n<p>整个图神经网络先将代表节点特征的<code class=\"language-text\">nodes</code>与边特征的<code class=\"language-text\">edges</code>按照每条数据拼接，形成了特征<code class=\"language-text\">cat_nodes</code>，该特征同时包含节点(图片+文字)与空间(边缘框)的特征。随后经过一个输出维度为1的全连接层并归一化，得到了残差系数矩阵(实际是[num_nodes, num_nodes, 1]的张量)<code class=\"language-text\">residual</code>，使用这个系数对某个node<code class=\"language-text\">i</code>计算与其他所有node之间特征的加权和，获得的残差值与输入的节点特征加和并作为新的节点特征输出。整个图神经网络迭代的过程就是不断地叠加这个残差值，使得最终对node的分类逼近真实值。</p>\n<p>其实把图神经网络换成普通的全连接层的话，这里就相当于是两个残差连接块。SDMGR在残差连接的基础上，使用了图神经网络块的方式融合节点与空间信息来构造残差。</p>\n<p>有趣的是论文原文中提到了SDMGR在处理图神经网络时候使用了动态注意力机制(dynamic attention mechanism)，也许这里的注意力机制指的就是在计算残差时对其他节点的特征进行了一个加权和，而这个权重是通过一个全连接层学习得到的。</p>\n<p>获得节点特征<code class=\"language-text\">nodes</code>和空间特征<code class=\"language-text\">cat_nodes</code>后，就可以对节点以及边分类了。经过全连接层，把<code class=\"language-text\">nodes</code>的维度从256调整到与分类数一致，cat_nodes的维度从256调整到2</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">node_cls<span class=\"token punctuation\">,</span> edge_cls <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>node_cls<span class=\"token punctuation\">(</span>nodes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>edge_cls<span class=\"token punctuation\">(</span>cat_nodes<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">return</span> node_cls<span class=\"token punctuation\">,</span> edge_cls</code></pre></div>\n<h1>5.总结</h1>\n<p>今天这篇文章从从代码角度介绍了整个SDMGR网络的词嵌入，循环神经网络，特征融合以及图神经网络部分。至此我们已经完整介绍了整个网络的结构，可以看到SDMGR将<code class=\"language-text\">U-Net</code>和<code class=\"language-text\">LSTM</code>作为Encoder抽取特征，图神经网络作为残差连接块，最后使用全连接层使用Decoder输出分类的思路是非常清晰的，在图神经网络模块还使用了动态注意力机制来学习文字区域之间的空间信息。</p>\n<p>下一篇文章会继续介绍损失函数与评估函数。</p>\n<p><a href=\"/paddle-ocr-kie-sdmgr-loss-and-evaluation\">关键信息提取网络SDMGR代码详解(4): 损失函数与模型评估</a></p>\n<h3>参考链接</h3>\n<ol>\n<li><a href=\"https://en.wikipedia.org/wiki/Long_short-term_memory\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://en.wikipedia.org/wiki/Long_short-term_memory</a></li>\n<li><a href=\"https://zh.m.wikipedia.org/zh/%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%E7%A7%AF\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://zh.m.wikipedia.org/zh/%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%E7%A7%AF</a></li>\n<li><a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://distill.pub/2021/gnn-intro/</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV1iT4y1d7zP\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.bilibili.com/video/BV1iT4y1d7zP</a></li>\n</ol>","excerpt":"在上篇文章关键信息提取网络SDMGR代码详解(2): 数据处理与主干网络中我们结合代码介绍了PaddleOCR中用于关键信息提取(KIE)任务网络SDMGR(Spatial Dual-Modality Graph Reasoning for Key Information Extraction)的主干网络部分。今天我们了解对文字部分处理的循环神经网络LSTM与融合多模态特征后进行图推理的图神经网络模块。而这两个模块也是SDMGR网络中最重要也是最复杂的模块。  这两部分的代码都位于。…","frontmatter":{"date":"July 12, 2022","path":"/paddle-ocr-kie-sdmgr-code-embedding-lstm-and-gnn","title":"关键信息提取网络SDMGR代码详解(3): 循环神经网络与图神经网络"}}},"pageContext":{}},"staticQueryHashes":["1176552510","3649515864","63159454","846684790"]}