{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/paddle-ocr-kie-sdmgr-code-data-process-and-backbone","result":{"data":{"markdownRemark":{"html":"<p>在上篇文章<a href=\"/paddle-ocr-kie-sdmgr-code-overview-and-application\">关键信息提取网络SDMGR代码详解(1): 概览与应用</a>中我们简单介绍了<a href=\"https://github.com/PaddlePaddle/PaddleOCR\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">PaddleOCR</a>中用于关键信息提取(KIE)任务网络<a href=\"https://arxiv.org/abs/2103.14470v1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SDMGR</a>(Spatial Dual-Modality Graph Reasoning for Key Information Extraction)并且手动尝试使用预训练模型对<a href=\"https://paperswithcode.com/dataset/wildreceipt\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">WildReceipt</a>数据集进行了推理。那这篇文章我们就从头开始对<code>SDMGR</code>网络的代码实现逐步解读一下吧。</p>\n<h1>1. 数据处理</h1>\n<p>回顾一下上篇文章中对<code>SDMGR</code>网络的训练/推理数据结构：每一行代表一条数据，格式为<code>图片位置\\t标号</code>。图片位置信息比较好理解，图片标号则是一个JSON Array, Array中每一项代表一条文字信息以及其位置信息。将这个JSON格式化后如下:</p>\n<pre><code class=\"language-json\">[\n    {\n        \"label\": 1,\n        \"transcription\": \"ILIO'S\",\n        \"points\": [\n            [\n                372.0,\n                242.0\n            ],\n            [\n                479.0,\n                242.0\n            ],\n            [\n                479.0,\n                178.0\n            ],\n            [\n                372.0,\n                178.0\n            ]\n        ]\n    },\n    ...\n]\n</code></pre>\n<p>那么SDMGR网络使用了“双模态”数据，即图片与文字标号，那么自然需要对这两种不同的数据进行预处理才能输入到神经网络中。在配置文件<code>configs/kie/kie_unet_sdmgr.yml</code>中的<code>Train.dataset.transforms</code>就定义了各个处理数据的步骤（训练和推理的预处理步骤几乎相同，这里先介绍训练的部分）。</p>\n<h2>1.1 DecodeImage</h2>\n<p>代码位置: <code>ppocr/data/imaug/operators.py#DecodeImage</code>。这步是标准的读取图片操作。参数<code>img_mode=RGB</code>和<code>channel_first=False</code>代表了以RGB三通道的方式使用<code>cv2</code>读取图片。最终图片的读取出图片的形状为<code>[1, 1, 3]</code>，即高度维，宽度维和通道维。</p>\n<h2>1.2 NormalizeImage</h2>\n<p>代码位置: <code>ppocr/data/imaug/operators.pyy#NormalizeImage</code>。这一步仅仅发生在训练阶段，从配置上来看这一步做的事是将图片RGB通道上的数据分别减去均值<code>[ 123.675, 116.28, 103.53 ]</code>除以标准差<code>[ 58.395, 57.12, 57.375 ]</code>。这里的参数<code>scale=1</code>，意味着各个像素的值保持了原始的0~255的大小减去均值除以标准差。</p>\n<pre><code class=\"language-python\">data['image'] = (img.astype('float32') * self.scale - self.mean) / self.std\n</code></pre>\n<p>那么这些“神奇”的数字是哪里来的，又为什么要进行图片的标准化呢？简单地说这些数值是数据集<a href=\"https://image-net.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ImageNet</a>上图片的和标准差。ImageNet是计算机视觉领域最著名的数据集之一，包含了超过1400万的图像和超过2万个对应的同义分类。有数量庞大的预训练模型是基于ImageNet数据集，ImageNet所对应的均值和标准差也自然而然地出现在各个地方。如果这里不传参数<code>scale=1</code>，那么每个像素的数值都会被除以255，拉到了0~1这个范围内。此时对应的标准差和均值则要除以255。分别为<code>[0.485, 0.456, 0.406]</code> 和 <code>[0.229, 0.224, 0.225]</code>，这也是ImageNet数据集均值和标准差的另一种著名形式。</p>\n<p>对图片做标准化可以将图片各个通道的数值分别拉到均值为0，方差为1的范围内。因为神经网络中的卷积层在前向计算时间会将各个通道的数值经过卷积核的卷积计算后进行加和输出到输出通道中，所以当两个通道的数据相差比较大时，反向传递梯度的时候也会导致梯度分配不均导致梯度方向不稳定。<strong>所以为了使得图片最终的均值为0，方差为1，这里实上要将减去的均值和除以的标准差替换为训练模型所使用数据的对应值，而不能直接使用ImageNet的均值和标准差。</strong> 关于卷积神经网络的图片标准化，更详细的内容可以参考<a href=\"https://zhuanlan.zhihu.com/p/35597976\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">这篇文章</a></p>\n<p>截至这一步，图片信息已经被处理为了形状为[1, 1, 3]，各个通道上均值为0方差为1的张量。</p>\n<h2>1.3 KieLabelEncode</h2>\n<p>代码位置: <code>ppocr/data/imaug/label_ops.py#KieLabelEncode</code>。这一步进入了对文字标签的处理，包含了空间信息和语义信息，所以代码会比较多，我们一步一步来看。</p>\n<p>首先，将标号的json字符串反序列化，处理空间的信息</p>\n<pre><code class=\"language-python\">def __call__(self, data):\n    import json\n    label = data['label']\n    # 将标号的json字符串反序列化\n    annotations = json.loads(label)\n    boxes, texts, text_inds, labels, edges = [], [], [], [], []\n    for ann in annotations:\n        # 获取标号中边界框的坐标\n        box = ann['points']\n        # 提取四个坐标的X轴值\n        x_list = [box[i][0] for i in range(4)]\n        # 提取四个坐标的Y轴值\n        y_list = [box[i][1] for i in range(4)]\n        # 从左上角开始，顺时针排序各个坐标点\n        sorted_x_list, sorted_y_list = self.sort_vertex(x_list, y_list)\n        sorted_box = []\n        # 重新将排序后X轴，Y轴数组转换为点的数组\n        for x, y in zip(sorted_x_list, sorted_y_list):\n            sorted_box.append(x)\n            sorted_box.append(y)\n        boxes.append(sorted_box)\n</code></pre>\n<p>获得排序好的边界框后，开始处理文字信息</p>\n<pre><code class=\"language-python\">      # 获取文字标号\n      text = ann['transcription']\n      texts.append(ann['transcription'])\n      # 将文字转换为字典文件中的序号，例如WildReceipt数据集的字典中，j将会被转换为数字49\n      text_ind = [self.dict[c] for c in text if c in self.dict]\n      text_inds.append(text_ind)\n      # 提取标号中的分类信息\n      if 'label' in ann.keys():\n          labels.append(ann['label'])\n      elif 'key_cls' in ann.keys():\n          labels.append(ann['key_cls'])\n      else:\n          raise ValueError(\"Cannot found 'key_cls' in ann.keys(), please check your training annotation.\")\n      # 提取标号中的edge信息，但实际WildReceipt数据集中并未提供该标号\n      edges.append(ann.get('edge', 0))\n  ann_infos = dict(\n      image=data['image'],\n      points=boxes,\n      texts=texts,\n      text_inds=text_inds,\n      edges=edges,\n      labels=labels)\n\n  return self.list_to_numpy(ann_infos)\n</code></pre>\n<p>对文字信息的处理获得了每个边界框对应的文字数组<code>texts</code>，转换为坐标的文字数组<code>text_inds</code>，分类信息数组<code>labels</code>和边信息数组<code>edges</code>(实际数据集中未给出)结合空间信息处理过的有序的边界框数组<code>boxes</code>，进入<code>list_to_numpy</code>方法进行进一步的处理。</p>\n<pre><code class=\"language-python\">def list_to_numpy(self, ann_infos):\n    \"\"\"Convert bboxes, relations, texts and labels to ndarray.\"\"\"\n    boxes, text_inds = ann_infos['points'], ann_infos['text_inds']\n    boxes = np.array(boxes, np.int32)\n    # 计算边界框之间的关系\n    relations, bboxes = self.compute_relation(boxes)\n\n    labels = ann_infos.get('labels', None)\n    if labels is not None:\n        labels = np.array(labels, np.int32)\n        edges = ann_infos.get('edges', None)\n        if edges is not None:\n            # 给labels添加一个新的维度，shape从[n]变为[n, 1]\n            labels = labels[:, None]\n            edges = np.array(edges)\n            # 将edge从一维数组变为一个n*n的矩阵，每一行代表这条边的标号是否与其他边相同。对角线为1\n            edges = (edges[:, None] == edges[None, :]).astype(np.int32)\n            if self.directed:\n                edges = (edges &#x26; labels == 1).astype(np.int32)\n            np.fill_diagonal(edges, -1)\n            # 将labels转换为列向量后和edges矩阵拼接，拼接后的形状为[n, n+1]\n            labels = np.concatenate([labels, edges], -1)\n    # 由于每个文字标号的长度都不一样，将每个文字标号的长度都调整为300，不足的用-1补齐。recoder_len是最长的文字长度。\n    padded_text_inds, recoder_len = self.pad_text_indices(text_inds)\n    max_num = 300\n    # 初始化[300, 4]的矩阵存放边界框\n    temp_bboxes = np.zeros([max_num, 4])\n    h, _ = bboxes.shape\n    # 将边界框填入上述的容器，不足的位置以0补齐\n    temp_bboxes[:h, :] = bboxes\n\n    # 初始化[300, 300, 5]的张量存放边界框之间的关系\n    temp_relations = np.zeros([max_num, max_num, 5])\n    # 填入关系\n    temp_relations[:h, :h, :] = relations\n\n    # 初始化[300, 300]的矩阵存放文字\n    temp_padded_text_inds = np.zeros([max_num, max_num])\n    # 填入文字\n    temp_padded_text_inds[:h, :] = padded_text_inds\n\n    # 初始化[300, 300]的矩阵存放标签(包括边信息)\n    temp_labels = np.zeros([max_num, max_num])\n    # 存入标签信息\n    temp_labels[:h, :h + 1] = labels\n\n    tag = np.array([h, recoder_len])\n    return dict(\n        image=ann_infos['image'],\n        points=temp_bboxes,\n        relations=temp_relations,\n        texts=temp_padded_text_inds,\n        labels=temp_labels,\n        tag=tag)\n</code></pre>\n<p>经过上面的代码，每一条数据的标号数量都被限制在300个并处理成了相同的形状。最终对于批量中的每一条数据，处理并转换成了以下的形式：</p>\n<ul>\n<li>image： 此步未被处理</li>\n<li>points： 形状为[300, 4]，第二个维度是每个边界框左上点的X,Y值和右下点的X,Y值</li>\n<li>relations：形状为[300, 300, 5]，前两个维度代表了每一条标号的边界框之间两两关系，最后一个维度5代表每个关系的特征数。关系特征是如何得到的将在下文解释</li>\n<li>texts：形状为[300, 300]，每个文字标号也被限制了300的长度</li>\n<li>labels：形状为[300, 300]，第一列为每个文字框的分类，后面的列为边与边之间的对应关系(由于WildReceipt数据集没有给edges，实际这里只有第一列)</li>\n<li>tag: 形状为[2], 第一个数字为该条数据总标号数量，第二个数字为标号中最长的文字长度。</li>\n</ul>\n<p>其中，relations的计算是SDMGR论文中的一个关键点，即如何将两个点之间的关系转化为一个向量表示呢？首先我们来看一下论文中的公式是如何描述的。</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 622px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/86d403c5da2947f6554558f196a2431a/5e4ef/relation.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 88.57142857142857%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAASABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHuKwaKikBoD//EABoQAAICAwAAAAAAAAAAAAAAAAARARASIDH/2gAIAQEAAQUCQqyHpHP/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAUEAEAAAAAAAAAAAAAAAAAAAAw/9oACAEBAAY/Ah//xAAbEAEAAQUBAAAAAAAAAAAAAAABABARITFBgf/aAAgBAQABPyF7zAazLW5FUisd+1v/2gAMAwEAAgADAAAAELDHAP/EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8QH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8QH//EAB4QAQACAgEFAAAAAAAAAAAAAAEAESExQXGBkaHw/9oACAEBAAE/EDpsnrFXbziVR7MyDCXWoIWqO0NVM3NnzbOSE4cz/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"relation\"\n        title=\"\"\n        src=\"/static/86d403c5da2947f6554558f196a2431a/5e4ef/relation.jpg\"\n        srcset=\"/static/86d403c5da2947f6554558f196a2431a/e52aa/relation.jpg 175w,\n/static/86d403c5da2947f6554558f196a2431a/70ebb/relation.jpg 350w,\n/static/86d403c5da2947f6554558f196a2431a/5e4ef/relation.jpg 622w\"\n        sizes=\"(max-width: 622px) 100vw, 622px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>然后我们结合代码来看一下</p>\n<pre><code class=\"language-python\">def compute_relation(self, boxes):\n    \"\"\"Compute relation between every two boxes.\"\"\"\n    # 所有边界框左上角点的X,Y轴值, 形状都是[n, 1]\n    x1s, y1s = boxes[:, 0:1], boxes[:, 1:2]\n    # 所有边界框右下角点的X,Y轴值, 形状都是[n, 1]\n    x2s, y2s = boxes[:, 4:5], boxes[:, 5:6]\n    # ws是同一个框右下X减去左上X，即宽度。+1防止ws是0\n    # hs是右下Y减去左上Y，但是最小值限制在1以上。\n    # ws和hs的形状都是[n, 1]\n    ws, hs = x2s - x1s + 1, np.maximum(y2s - y1s + 1, 1)\n    # self.norm为常数10\n    # x1s[:, 0][None]的形状是[1, n]减去x1s利用了广播机制，变为了[n, n]的矩阵，dxs代表了所有框两两之间X轴的距离。\n    # 同样的，dys代表了Y轴上的记录，他们都除以了归一化常数10.\n    dxs = (x1s[:, 0][None] - x1s) / self.norm\n    dys = (y1s[:, 0][None] - y1s) / self.norm\n    # 同样的， hs[:, 0][None] / hs利用广播机制形成了[n, n]的矩阵，xhhs代表了所有框两两之间的高度比\n    # 而xwhs则代表两两之间的宽高比\n    xhhs, xwhs = hs[:, 0][None] / hs, ws[:, 0][None] / hs\n    # whs则代表了当前框自身的宽高比\n    whs = ws / hs + np.zeros_like(xhhs)\n    # 将上述五个特征按每两个边界框之间的关系放入一个数组，形成shape为[n, n, 5]的张量\n    relations = np.stack([dxs, dys, whs, xhhs, xwhs], -1)\n    # 将左上X,Y,右下X,Y作为一组向量代表一个边界框，形状为[n, 4]\n    bboxes = np.concatenate([x1s, y1s, x2s, y2s], -1).astype(np.float32)\n    return relations, bboxes\n</code></pre>\n<p>从上面的代码可以知道，边界框<code>i</code>与另一个边界框<code>j</code>之间的relation被描述为一个长度为5的向量，这些特征分别为：</p>\n<ul>\n<li>dxs: 两框之间X轴的距离</li>\n<li>dys：两框之间Y轴的距离</li>\n<li>whs：边界框<code>i</code>自身的宽高比</li>\n<li>xhhs：边界框<code>j</code>与边界框<code>i</code>的高度比</li>\n<li>xwhs：边界框<code>j</code>的宽度与边界框<code>i</code>的高度比</li>\n</ul>\n<p>到此为止，一个数据点的文字内容和空间信息也被转化为可以输入神经网络的张量了。</p>\n<h2>1.4 KieResize, ToCHWImage, KeepKeys</h2>\n<p>这三步相对比较简单：</p>\n<ul>\n<li>KieResize：代码位置<code>ppocr/data/imaug/operators.py#KieResize</code>，将图片压缩按比例压缩至长边不大于1024像素，短边不大于512像素。同时会压缩边界框</li>\n<li>ToCHWImage：代码位置<code>ppocr/data/imaug/operators.py#ToCHWImage</code>，将图片的通道维变为第一维，形状从[h, w, c]变成[c, h, w]。</li>\n<li>KeepKeys: 代码位置<code>ppocr/data/imaug/operators.py#KeepKeys</code>，对处理完成的数据保留指定的字段，并按顺序转换为一个数组。</li>\n</ul>\n<h1>2. 主干网络</h1>\n<p>数据处理完成后就可以进入神经网络了。第一步是将图片和文字部分分别放入<code>U-Net</code>和<code>LSTM</code>进行特征提取。结合SDMGR的网络结构看，就是下图中左侧\"Dual Modality Fusion Module\"中的上下两部分了。这篇文章按照代码结构，先介绍图片处理的部分。</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/9f2841aa6e29db8849739d5cc5f853a4/56873/sdmgr-net.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.28571428571429%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAIBAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHbZAsID//EABYQAQEBAAAAAAAAAAAAAAAAAAABQf/aAAgBAQABBQLUaj//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAADAAAAAAAAAAAAAAAAAAAAIDH/2gAIAQEABj8CKv8A/8QAHBAAAgEFAQAAAAAAAAAAAAAAABEhARAxQWGh/9oACAEBAAE/IVKfR8DKas2tif/aAAwDAQACAAMAAAAQwM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAcEAADAAEFAAAAAAAAAAAAAAAAARExECFhcbH/2gAIAQEAAT8QhWdV8Cg3kGuUe+mXs//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"sdmgr-net\"\n        title=\"\"\n        src=\"/static/9f2841aa6e29db8849739d5cc5f853a4/29d31/sdmgr-net.jpg\"\n        srcset=\"/static/9f2841aa6e29db8849739d5cc5f853a4/e52aa/sdmgr-net.jpg 175w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/70ebb/sdmgr-net.jpg 350w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/29d31/sdmgr-net.jpg 700w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/9ecec/sdmgr-net.jpg 1050w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/d165a/sdmgr-net.jpg 1400w,\n/static/9f2841aa6e29db8849739d5cc5f853a4/56873/sdmgr-net.jpg 2332w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>代码位置<code>ppocr/modeling/backbones/kie_unet_sdmgr.py</code></p>\n<h2>2.1 U-Net</h2>\n<p>我们先来看下图片特征抽取的前向运算是如何定义的</p>\n<pre><code class=\"language-python\">def forward(self, inputs):\n    img = inputs[0]\n    relations, texts, gt_bboxes, tag, img_size = inputs[1], inputs[\n        2], inputs[3], inputs[5], inputs[-1]\n    # 预处理，将各个变量转换为paddle.Tensor\n    img, relations, texts, gt_bboxes = self.pre_process(\n        img, relations, texts, gt_bboxes, tag, img_size)\n    x = self.img_feat(img)\n    boxes, rois_num = self.bbox2roi(gt_bboxes)\n    # 对图片输出进行ROI Pooling\n    feats = paddle.vision.ops.roi_align(\n        x, boxes, spatial_scale=1.0, output_size=7, boxes_num=rois_num)\n    feats = self.maxpool(feats).squeeze(-1).squeeze(-1)\n    return [relations, texts, feats]\n</code></pre>\n<p>在获得各个输入的<code>paddle.Tensor</code>后，图片进入了self.img_feat，也就是U-Net进行特征抽取。</p>\n<p><a href=\"https://arxiv.org/abs/1505.04597\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">U-Net</a>本身也是一个十分值得学习的卷积神经网络，但因他不是本文的重点，这里我们结合代码简单看一下吧。</p>\n<pre><code class=\"language-python\">class UNet(nn.Layer):\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.down1 = Encoder(num_channels=3, num_filters=16)\n        self.down2 = Encoder(num_channels=16, num_filters=32)\n        self.down3 = Encoder(num_channels=32, num_filters=64)\n        self.down4 = Encoder(num_channels=64, num_filters=128)\n        self.down5 = Encoder(num_channels=128, num_filters=256)\n\n        self.up1 = Decoder(32, 16)\n        self.up2 = Decoder(64, 32)\n        self.up3 = Decoder(128, 64)\n        self.up4 = Decoder(256, 128)\n        self.out_channels = 16\n\n    def forward(self, inputs):\n        x1, _ = self.down1(inputs)\n        _, x2 = self.down2(x1)\n        _, x3 = self.down3(x2)\n        _, x4 = self.down4(x3)\n        _, x5 = self.down5(x4)\n\n        x = self.up4(x4, x5)\n        x = self.up3(x3, x)\n        x = self.up2(x2, x)\n        x = self.up1(x1, x)\n        return x\n</code></pre>\n<p>可以清晰地看到整个网络分为<code>Encoder</code>和<code>Decoder</code>两部分。其中前半部分Encode地过程中，输入图片的通道数逐渐从3增大到256，每一个<code>Encoder</code>块结构为</p>\n<ul>\n<li>卷积层：kernel_size=3, stride=1, padding=1, 通道数由输入通道数变为输出通道数</li>\n<li>BN层 + ReLU激活函数</li>\n<li>卷积层：kernel_size=3, stride=1, padding=1, 通道数不变</li>\n<li>BN层 + ReLU激活函数</li>\n<li>最大池化层：kernel_size=3, stride=2, padding=1</li>\n</ul>\n<p>由于最大池化层的步长为2，每经过一个Encoder块，图片的长款就会各减少一半。SDMGR用了5个Encode，但由于第一个Encoder块实际没有取池化层的输出，图片的长宽减少了16倍，通道数从3增加到了256。</p>\n<p>每一个<code>Decoder</code>块的结构为</p>\n<ul>\n<li>卷积层：kernel_size=3, stride=1, padding=1, 通道数由输入通道数变为输出通道数</li>\n<li>BN层 + ReLU激活函数</li>\n<li>双线性插值长宽扩大2倍</li>\n<li>在通道维拼接对应Encoder的输出, 使得通道数加倍，回到了输入通道数</li>\n<li>卷积层：kernel_size=3, stride=1, padding=1, 通道数由输入通道数变为输出通道数</li>\n<li>BN层 + ReLU激活函数</li>\n<li>卷积层：kernel_size=1, stride=1, padding=0, 通道数不变</li>\n<li>BN层 + ReLU激活函数</li>\n</ul>\n<p>简单的说，每个Decoder块先通过一个卷积层，把输入的通道数减半为输出通道数，然后将对应Encoder块输出(对应Encoder块的输出通道数等于Decoder块的输出通道数)在通道维拼接，通道数加倍回到了输入通道的数量。然后再过一个卷积层通道减半，又把通道数变为了输出通道数。每经过一个Decoder块则输出通道数减半，图片的长宽各翻倍。经过了4个Decoder块后，最终图片的尺寸回到了原始图片的大小，通道数为16。即原图中的每个像素获得了一个长度为16的特征。</p>\n<p>本文的U-Net实现与U-Net的论文有略微的区别，因为两者的目的不同。原始的U-Net最终输出的通道数为2，目的是对图片进行语义分割，将图片中的像素进行二分类，而本文仅仅将U-Net用作对图片的特征提取。</p>\n<h2>2.2 ROI Pooling</h2>\n<p>ROI Pooling(Region of interest pooling)的目的是将整个图片输出的大特征图切割成对应各个边界框的小特征图集合。第一步就是从边界框信息中提取ROI区域的大小</p>\n<pre><code class=\"language-python\">def bbox2roi(self, bbox_list):\n    rois_list = []\n    rois_num = []\n    for img_id, bboxes in enumerate(bbox_list):\n        # roi数量与实际的边界框数相同\n        rois_num.append(bboxes.shape[0])\n        # 每个roi与边界框一样大\n        rois_list.append(bboxes)\n    rois = paddle.concat(rois_list, 0)\n    rois_num = paddle.to_tensor(rois_num, dtype='int32')\n    return rois, rois_num\n</code></pre>\n<p>最后，将每个边界框对应的ROI应用到图片提取出的特征图中，相当于获取了每个边界框对应的特征图区域。</p>\n<pre><code class=\"language-python\">feats = paddle.vision.ops.roi_align(\n    x, boxes, spatial_scale=1.0, output_size=7, boxes_num=rois_num)\n</code></pre>\n<p>ROI池化完成后，便完成了每个边界框与图片抽出的特征图中对应区域的映射，如论文中下图所示，完成了文字，边界框与图片信息的特征“三位一体”。只不过此时图片并不是原始的RGB三通道图片，而是抽取特征后的16通道特征图了。</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 565px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f8d400a09f72c1d583c8b6611d5fb1bb/07eba/roi.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.85714285714286%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAA7DAAAOwwHHb6hkAAACdklEQVR42m2T61MSYRTG+6OzLBBFGFTUDBBBp6s1TU06jn5wykbTykhBQWG5CSq3zMuEIAiC7K+zi2bTeGafPfPOOc9zLvvuHdQm/FqA3aew9xx1V7D3AjUl58STNpKC3Wd6/FYIh9MQmt3hLCmkx7DjlcA4pdAI9YQH1dfP2ZKJ8tdeqkvdsDEs8QkR90juvxBe0g1piV1WRbAUhrhDDzaiLsqbdgrrLnyTNoJvO1l/byQ2ZQT/IGrai5oYk+5FSHDtSYjgzjg0TjXBCMQeSZVRmlGnCA5wEhhhadrLyrsJ5l69JDAryVFtCpeQHagbdtgcEi9FNH/VUFvwVDqMDoqgk1pkmJZ4Uk4uYtJtZIyfAS/lsJti0E01Mkrxh4PDmU7y8x3sz3VQ/2Dk6IuZZnwUmrrgtgj2c7FtZ2/RwInPzFlwEGXZS8bnJrc+RnZNE3az832cT9MeFiedbMx6WX0zwvFCLwfLBoqBAdlhSRPcAsXCZbiPetDKRchKS7HJrux6odKKlcy89oF6ZDwLNcVO3u+mEPCQW3NRV4ZkDX0y9vD1yEGI9MgebW3EbTTCFnKfO8gudpGdMZGdf8j+1H1Y7ZJCWl6vwCy54hUrasQigrK2RlET3ITtexIwoyoiHJWR/TL6t7skFgx8fO3AP+MhNNUHQSNqVMu7ARonYtJ5NLWR5UVKvlL4gQQMbUTlmsS7ZBSTdNRNKy7khBDjQlSMN3k6tHMnFKavLrZmlxU4zwiyOtRqhlZln8pxjMpRlGIhxO/8FuVDhUYpDbXc31ydVyug/3FtQVUelf+t1WpRKByQyx8QTyRR4ilSaSlSOedWu9L4AyyvOjf3DgupAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"roi\"\n        title=\"\"\n        src=\"/static/f8d400a09f72c1d583c8b6611d5fb1bb/07eba/roi.png\"\n        srcset=\"/static/f8d400a09f72c1d583c8b6611d5fb1bb/4edbd/roi.png 175w,\n/static/f8d400a09f72c1d583c8b6611d5fb1bb/13ae7/roi.png 350w,\n/static/f8d400a09f72c1d583c8b6611d5fb1bb/07eba/roi.png 565w\"\n        sizes=\"(max-width: 565px) 100vw, 565px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h1>5.总结</h1>\n<p>今天这篇文章从从代码角度介绍了整个SDMGR网络的数据处理和图片特征抽取部分。下一篇文章会继续介绍文字特征抽取，特征融合与图神经网络模块。</p>\n<p><a href=\"/paddle-ocr-kie-sdmgr-code-embedding-lstm-and-gnn\">关键信息提取网络SDMGR代码详解(3): 循环神经网络与图神经网络</a></p>\n<h3>参考链接</h3>\n<ol>\n<li><a href=\"https://arxiv.org/abs/2103.14470v1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/abs/2103.14470v1</a></li>\n<li><a href=\"https://image-net.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://image-net.org/</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/35597976\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://zhuanlan.zhihu.com/p/35597976</a></li>\n<li><a href=\"https://arxiv.org/abs/1505.04597\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/abs/1505.04597</a></li>\n</ol>","excerpt":"在上篇文章关键信息提取网络SDMGR代码详解(1): 概览与应用中我们简单介绍了PaddleOCR中用于关键信息提取(KIE)任务网络SDMGR(Spatial Dual-Modality Graph Reasoning for Key Information Extraction)并且手动尝试使用预训练模型对WildReceipt数据集进行了推理。那这篇文章我们就从头开始对SDMGR网络的代码实现逐步解读一下吧。 1. 数据处理 回顾一下上篇文章中对SDMGR…","frontmatter":{"date":"July 06, 2022","path":"/paddle-ocr-kie-sdmgr-code-data-process-and-backbone","title":"关键信息提取网络SDMGR代码详解(2): 数据处理与主干网络"}}},"pageContext":{}},"staticQueryHashes":["2560569871","3584596544","63159454"],"slicesMap":{}}