{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/cuda-gpu-setup-for-paddle-on-windows-wsl","result":{"data":{"markdownRemark":{"html":"<h1>TL;DR</h1>\n<p><a href=\"https://github.com/PaddlePaddle/Paddle\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">PaddlePaddle</a>是百度出品的深度学习框架。基于PaddlePaddle百度还推出了<a href=\"https://github.com/PaddlePaddle/PaddleOCR\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">PaddleOCR</a>，<a href=\"https://github.com/PaddlePaddle/PaddleNLP\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">PaddleNLP</a>，<a href=\"https://github.com/PaddlePaddle/PaddleHub\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">PaddleHub</a>等实用的工具。作为深度学习框架，<code class=\"language-text\">PaddlePaddle</code>自然也在多个平台支持基于GPU的模型训练和推理，其中也包括Windows平台。如果你想使用Windows系统训练神经网络模型，而又想获得Linux的开发体验的话，Windows Subsystem for Linux (<code class=\"language-text\">WSL</code>)显然是一个不错的选择。如今英伟达的官方显卡驱动官方支持了在WSL系统内部调用CUDA，本文介绍了如何从零开始搭建基于WSL和<code class=\"language-text\">PaddlePaddle</code>的GPU深度学习环境。</p>\n<p>本文环境: <code class=\"language-text\">System: Windows 10, version 21H2</code>, <code class=\"language-text\">GPU: GTX 3070</code>, <code class=\"language-text\">nVidia driver: 516.01</code></p>\n<h1>1. 安装WSL</h1>\n<p>在安装WSL之前需要注意的是，如果想在WSL内使用CUDA，需要确保Windows版本是<code class=\"language-text\">Windows 11</code>或者至少<code class=\"language-text\">Windows 10, version 21H2</code>。具体的要求可以参考<a href=\"https://docs.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">官方文档</a>。而显卡驱动方面则是要安装包含WSL CUDA驱动的<a href=\"https://www.nvidia.com/download/index.aspx\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">官方驱动</a></p>\n<p>打开管理员权限的PowerShell输入一下命令即可安装WSL，目前默认安装的版本是<code class=\"language-text\">Ubuntu 20.04 LTS</code>。需要注意的是目前为了在WSL里使用CUDA,需要安装基于<code class=\"language-text\">glibc</code>的Linux发行，默认的Ubuntu是满足要求的。</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">wsl --install</code></pre></div>\n<p>安装完毕后使用<code class=\"language-text\">wsl --status</code>命令可获取当前WSL的安装信息，需要确保安装的版本是WSL2，如果仍是WSL1.X，可使用<code class=\"language-text\">wsl --set-default-version 2</code>命令将默认版本切换至WSL2后重新安装。</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">></span> wsl --status\n<span class=\"token comment\"># 默认分发：Ubuntu</span>\n<span class=\"token comment\"># 默认版本：2</span></code></pre></div>\n<p>WSL安装完毕后进入Linux子系统，此时因为宿主机已经安装了WSL显卡驱动，在wsl内部输入<code class=\"language-text\">nvidia-smi</code>已经可以获取GPU信息</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">></span> nvidia-smi\n<span class=\"token comment\"># +-----------------------------------------------------------------------------+</span>\n<span class=\"token comment\"># | NVIDIA-SMI 515.43.04    Driver Version: 516.01       CUDA Version: 11.7     |</span>\n<span class=\"token comment\"># |-------------------------------+----------------------+----------------------+</span>\n<span class=\"token comment\"># | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>\n<span class=\"token comment\"># | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>\n<span class=\"token comment\"># |                               |                      |               MIG M. |</span>\n<span class=\"token comment\"># |===============================+======================+======================|</span>\n<span class=\"token comment\"># |   0  NVIDIA GeForce ...  On   | 00000000:08:00.0  On |                  N/A |</span>\n<span class=\"token comment\"># |  0%   47C    P8    11W / 198W |   1672MiB /  8192MiB |      2%      Default |</span>\n<span class=\"token comment\"># |                               |                      |                  N/A |</span>\n<span class=\"token comment\"># +-------------------------------+----------------------+----------------------+</span>\n                                                                               \n<span class=\"token comment\"># +-----------------------------------------------------------------------------+</span>\n<span class=\"token comment\"># | Processes:                                                                  |</span>\n<span class=\"token comment\"># |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span>\n<span class=\"token comment\"># |        ID   ID                                                   Usage      |</span>\n<span class=\"token comment\"># |=============================================================================|</span>\n<span class=\"token comment\"># |  No running processes found                                                 |</span>\n<span class=\"token comment\"># +-----------------------------------------------------------------------------+</span></code></pre></div>\n<h1>2. 安装CUDA Toolkit</h1>\n<p><a href=\"https://developer.nvidia.com/cuda-toolkit\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">CUDA Toolkit</a>是利用CUDA进行GPU加速的开发环境, 而<code class=\"language-text\">CUDA</code>(ComputeUnified Device Architecture)是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。根据<a href=\"https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started-with-cuda-on-wsl\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">官方文档</a>，Windows下安装的GPU驱动程序已经集成了默认的CUDA Toolkit，但为了避免宿主机驱动更改对WSL环境产生影响，最好在WSL内部独立安装一套CUDA Toolkit。</p>\n<p>首先需要删除老的GPG key</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">sudo</span> apt-key del 7fa2af80</code></pre></div>\n<p>然后使用命令安装，由于本机的GPU是安培架构的，以下示例是基于CUDA 11.7版本的。<code class=\"language-text\">PaddlePaddle</code>对非安培架构推荐安装CUDA 10.2，其他版本安装地址可以<a href=\"https://developer.nvidia.com/cuda-downloads?target_os=Linux&#x26;target_arch=x86_64&#x26;Distribution=WSL-Ubuntu&#x26;target_version=2.0\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">这里</a>查询。</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># 下载并安装apt-pinning文件，</span>\n<span class=\"token function\">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\n<span class=\"token function\">sudo</span> <span class=\"token function\">mv</span> cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\n<span class=\"token comment\"># 下载并安装CUDA Toolkit wsl版本地安装包，需要将版本替换为你希望安装的CUDA版本</span>\n<span class=\"token function\">wget</span> https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda-repo-wsl-ubuntu-11-7-local_11.7.0-1_amd64.deb\n<span class=\"token function\">sudo</span> dpkg -i cuda-repo-wsl-ubuntu-11-7-local_11.7.0-1_amd64.deb</code></pre></div>\n<p>安装完deb包后如果出现如下提示，那么请按照提示安装GPG key</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">The public CUDA GPG key does not appear to be installed.\nTo install the key, run this command:\nsudo cp /var/cuda-repo-wsl-ubuntu-11-7-local/cuda-B81839D3-keyring.gpg /usr/share/keyrings/</code></pre></div>\n<p>最后使用<code class=\"language-text\">apt-get</code>安装CUDA Toolkit</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> update\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> -y <span class=\"token function\">install</span> cuda</code></pre></div>\n<h1>3. 安装cuDNN</h1>\n<p><a href=\"https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">cuDNN</a>(NVIDIA® CUDA® Deep Neural Network library)是用于深度神经网络的GPU加速库。除了<code class=\"language-text\">PaddlePaddle</code>外，几乎各大深度学习框架如<code class=\"language-text\">Tensorflow</code>，<code class=\"language-text\">PyTorch</code>均支持<code class=\"language-text\">cuDNN</code>。</p>\n<p>在安装<code class=\"language-text\">cuDNN</code>之前，需要先安装<code class=\"language-text\">zlib</code>用于压缩/解压</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> <span class=\"token function\">install</span> zlib1g</code></pre></div>\n<p>而<code class=\"language-text\">cuDNN</code>本身的安装文件需要去<a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">官网</a>手动下载，因为英伟达要求在下载cuDNN之前先注册账号，并给下载链接签发一个绑定账号的token😓在下载页面我们选择<code class=\"language-text\">Local Installer for Linux x86_64 (Tar)</code></p>\n<p>假设我们下载的压缩文件叫<code class=\"language-text\">cudnn-linux-x86_64-8.4.1.50_cuda11.6-archive.tar.xz</code>，下载完成后先解压</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">tar</span> -xvf cudnn-linux-x86_64-8.4.1.50_cuda11.6-archive.tar.xz</code></pre></div>\n<p>然后将对应的文件拷贝到<code class=\"language-text\">/usr/local/cuda</code>下</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">sudo</span> <span class=\"token function\">cp</span> cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include \n<span class=\"token function\">sudo</span> <span class=\"token function\">cp</span> -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 \n<span class=\"token function\">sudo</span> <span class=\"token function\">chmod</span> a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*</code></pre></div>\n<p>至此<code class=\"language-text\">cuDNN</code>已经安装成功了。</p>\n<h1>4. 利用Conda安装Python</h1>\n<p><code class=\"language-text\">Conda</code>是Python的版本和环境管理工具，有些像NodeJS的<code class=\"language-text\">NVM</code>和Ruby的<code class=\"language-text\">RVM</code>。Conda分为Miniconda和Anaconda，前者仅包含基础内容，而后者包含了一些常用的包在内。以Anaconda为例，在<a href=\"https://www.anaconda.com/products/distribution\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">官网</a>可以找到下载链接，实际是一段脚本，可以下载执行安装</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> -o- https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh <span class=\"token operator\">|</span> <span class=\"token function\">bash</span></code></pre></div>\n<p>安装完后重新打开WSL终端就可以获得Python环境(目前默认是3.9)，也可以创建新的Conda环境以及在不同版本的Python间切换。介绍Conda使用的文章有很多，这里不在赘述，可以参考<a href=\"https://www.jianshu.com/p/edaa744ea47d\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">conda的安装与使用</a></p>\n<h1>5. 安装PaddlePaddle环境</h1>\n<p>PaddlePaddle有CPU版和GPU版，我们这次自然是要安装GPU版本。从<a href=\"https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/linux-pip.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">官网</a>可以获取使用<code class=\"language-text\">pip</code>或者<code class=\"language-text\">conda</code>安装不同版本的指令。以目前CUDA 11.7版为例，虽然PaddlePaddle当前最高支持CUDA 11.2，但CUDA 11.7对其仍然兼容，所以我们利用pip安装基于CUDA 11.2的<code class=\"language-text\">paddlepaddle-gpu</code>包即可。</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">pip <span class=\"token function\">install</span> paddlepaddle-gpu<span class=\"token operator\">==</span><span class=\"token number\">2.3</span>.0.post112 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html</code></pre></div>\n<p>为了验证安装，我们在WSL输入<code class=\"language-text\">python</code>进入交互式Python环境，输入<code class=\"language-text\">import paddle</code> ，回车后再输入<code class=\"language-text\">paddle.utils.run_check()</code>。如果打印信息显示</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Python <span class=\"token number\">3.9</span><span class=\"token number\">.12</span> <span class=\"token punctuation\">(</span>main<span class=\"token punctuation\">,</span> Apr  <span class=\"token number\">5</span> <span class=\"token number\">2022</span><span class=\"token punctuation\">,</span> <span class=\"token number\">06</span><span class=\"token punctuation\">:</span><span class=\"token number\">56</span><span class=\"token punctuation\">:</span><span class=\"token number\">58</span><span class=\"token punctuation\">)</span> \n<span class=\"token punctuation\">[</span>GCC <span class=\"token number\">7.5</span><span class=\"token number\">.0</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span> Anaconda<span class=\"token punctuation\">,</span> Inc<span class=\"token punctuation\">.</span> on linux\nType <span class=\"token string\">\"help\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"copyright\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"credits\"</span> <span class=\"token keyword\">or</span> <span class=\"token string\">\"license\"</span> <span class=\"token keyword\">for</span> more information<span class=\"token punctuation\">.</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">import</span> paddle\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> paddle<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>run_check<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nRunning verify PaddlePaddle program <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span> \nW0614 <span class=\"token number\">17</span><span class=\"token punctuation\">:</span><span class=\"token number\">12</span><span class=\"token punctuation\">:</span><span class=\"token number\">36.941226</span>  <span class=\"token number\">2519</span> gpu_context<span class=\"token punctuation\">.</span>cc<span class=\"token punctuation\">:</span><span class=\"token number\">278</span><span class=\"token punctuation\">]</span> Please NOTE<span class=\"token punctuation\">:</span> device<span class=\"token punctuation\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> GPU Compute Capability<span class=\"token punctuation\">:</span> <span class=\"token number\">8.6</span><span class=\"token punctuation\">,</span> Driver API Version<span class=\"token punctuation\">:</span> <span class=\"token number\">11.7</span><span class=\"token punctuation\">,</span> Runtime API Version<span class=\"token punctuation\">:</span> <span class=\"token number\">11.2</span>\nW0614 <span class=\"token number\">17</span><span class=\"token punctuation\">:</span><span class=\"token number\">12</span><span class=\"token punctuation\">:</span><span class=\"token number\">36.967586</span>  <span class=\"token number\">2519</span> gpu_context<span class=\"token punctuation\">.</span>cc<span class=\"token punctuation\">:</span><span class=\"token number\">306</span><span class=\"token punctuation\">]</span> device<span class=\"token punctuation\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> cuDNN Version<span class=\"token punctuation\">:</span> <span class=\"token number\">8.4</span><span class=\"token punctuation\">.</span>\nPaddlePaddle works well on <span class=\"token number\">1</span> GPU<span class=\"token punctuation\">.</span>\nPaddlePaddle works well on <span class=\"token number\">1</span> GPUs<span class=\"token punctuation\">.</span>\nPaddlePaddle <span class=\"token keyword\">is</span> installed successfully! Let's start deep learning <span class=\"token keyword\">with</span> PaddlePaddle now<span class=\"token punctuation\">.</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> </code></pre></div>\n<p>那么代表PaddlePaddle GPU环境已经准备就绪，可以在显卡上进行深度学习的工作了。但如果出现某些文件缺失，比如这里提示<code class=\"language-text\">libcuda.so</code>找不到</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">The third-party dynamic library (libcuda.so) that Paddle depends on is not configured correctly. (error code is libcuda.so: cannot open shared object file: No such file or directory)</code></pre></div>\n<p>那很有可能是某些依赖文件通过WSL注入Linux环境后，没有被加入动态/共享库的路径中。</p>\n<p>首先在<code class=\"language-text\">/usr</code>路径下寻找该文件</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">></span> <span class=\"token function\">sudo</span> <span class=\"token function\">sudo</span> <span class=\"token function\">find</span> /usr/ -name <span class=\"token string\">'libcuda.so'</span>\n<span class=\"token comment\"># /usr/lib/wsl/lib/libcuda.so</span></code></pre></div>\n<p>可以看到这个依赖其实已经在<code class=\"language-text\">/usr/lib/wsl/lib/libcuda.so</code>这个路径下了，只是PaddlePaddle无法从<code class=\"language-text\">LD_LIBRARY_PATH</code>中找到它。那只需要把<code class=\"language-text\">/usr/lib/wsl/lib</code>这个路径加入到<code class=\"language-text\">LD_LIBRARY_PATH</code>即可。</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">LD_LIBRARY_PATH</span><span class=\"token operator\">=</span>/usr/lib/wsl/lib:<span class=\"token variable\">$LD_LIBRARY_PATH</span></code></pre></div>\n<h1>5. 使用PaddleOCR验证GPU环境</h1>\n<p><a href=\"https://github.com/PaddlePaddle/PaddleOCR\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">PaddleOCR</a>是百度的开源OCR工具库，我们现在可以使用GPU跑一个PaddleOCR的端对端OCR推理来验证一下环境是否搭建成功。</p>\n<p>首先将PaddleOCR拉到本地并进入项目目录</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">git</span> clone git@github.com:PaddlePaddle/PaddleOCR.git\n\n<span class=\"token builtin class-name\">cd</span> PaddleOCR</code></pre></div>\n<p>PaddleOCR中有包括目标检测，文字识别，关键信息提取等诸多工具。我们这次使用最直观的端到端OCR，即输入图片，输出文字的模型。相关的说明文档在这个位置<code class=\"language-text\">doc/doc_ch/algorithm_e2e_pgnet.md</code></p>\n<p>首先需要下载与训练好的模型：</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">mkdir</span> inference <span class=\"token operator\">&amp;&amp;</span> <span class=\"token builtin class-name\">cd</span> inference\n<span class=\"token comment\"># 下载英文端到端模型并解压</span>\n<span class=\"token function\">wget</span> https://paddleocr.bj.bcebos.com/dygraph_v2.0/pgnet/e2e_server_pgnetA_infer.tar <span class=\"token operator\">&amp;&amp;</span> <span class=\"token function\">tar</span> xf e2e_server_pgnetA_infer.tar</code></pre></div>\n<p>解压后的文件目录结构为：</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">├── e2e_server_pgnetA_infer\n│   ├── inference.pdiparams\n│   ├── inference.pdiparams.info\n│   └── inference.pdmodel</code></pre></div>\n<p>其中比较关键的两个文件为<code class=\"language-text\">inference.pdmodel</code>以及<code class=\"language-text\">inference.pdiparams</code>，前者储存了整个神经网络模型的结构，而后者储存了各层训练完后的权重参数。如果你想可视化地浏览整个文件，可以使用<a href=\"https://netron.app/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Netron</a>这个工具打开这两个文件，整个模型的结构就一览无余了。\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 32.57142857142857%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAAA1UlEQVQoz42RSw6DMAxEcyJAgnyIRGynVbvi/sdxbZOAuuhnMQpxzPhl4gCAVQiFEZGRkHvtm7R3XVdeloWnaeJhGHgcR3Ykh0SVN3qKmazYjH+IiDjnzCEEM53n2eSwTQOsx1r+M+yEMUaTGqsc1IeRQW2NMrkTfJKe11rNsBt5702uGBlxaRn2fBA1107f69f+IExmpoSnoRKCEhKcV6nyo9YL3Q9yHSbfhbRXzYsQ3jjmjYOYpJSuKxegRghv+YDUTNBJrz20hwwxsZcH2ffdpDG8AKU18PFJz0OdAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='Netron' title='Netron' src='/static/3d2ca5a3f3b575f8b573fa212b0f24dd/8c557/netron.png' srcset='/static/3d2ca5a3f3b575f8b573fa212b0f24dd/4edbd/netron.png 175w,\n/static/3d2ca5a3f3b575f8b573fa212b0f24dd/13ae7/netron.png 350w,\n/static/3d2ca5a3f3b575f8b573fa212b0f24dd/8c557/netron.png 700w,\n/static/3d2ca5a3f3b575f8b573fa212b0f24dd/e996b/netron.png 1050w,\n/static/3d2ca5a3f3b575f8b573fa212b0f24dd/2cefc/netron.png 1400w,\n/static/3d2ca5a3f3b575f8b573fa212b0f24dd/e0dbc/netron.png 3378w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n    </span></p>\n<p>下载完模型后，回到PaddleOCR的根目录，就可以使用官方提供的工具对测试数据进行批量识别了:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">></span> python3 tools/infer/predict_e2e.py --e2e_algorithm<span class=\"token operator\">=</span><span class=\"token string\">\"PGNet\"</span> --image_dir<span class=\"token operator\">=</span><span class=\"token string\">\"./doc/imgs_en/\"</span> --e2e_model_dir<span class=\"token operator\">=</span><span class=\"token string\">\"./inference/e2e_server_pgnetA_infer/\"</span> --e2e_pgnet_valid_set<span class=\"token operator\">=</span><span class=\"token string\">\"totaltext\"</span>\n<span class=\"token comment\"># [2022/06/16 16:24:31] ppocr INFO: Predict time of ./doc/imgs_en/254.jpg: 5.411184072494507</span>\n<span class=\"token comment\"># [2022/06/16 16:24:31] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_254.jpg</span>\n<span class=\"token comment\"># [2022/06/16 16:24:31] ppocr INFO: Predict time of ./doc/imgs_en/img623.jpg: 0.08095073699951172</span>\n<span class=\"token comment\"># [2022/06/16 16:24:31] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_img623.jpg</span>\n<span class=\"token comment\"># [2022/06/16 16:24:31] ppocr INFO: Predict time of ./doc/imgs_en/img_10.jpg: 0.09116768836975098</span>\n<span class=\"token comment\"># [2022/06/16 16:24:31] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_img_10.jpg</span>\n<span class=\"token comment\"># [2022/06/16 16:24:31] ppocr INFO: Predict time of ./doc/imgs_en/img_11.jpg: 0.07429361343383789</span>\n<span class=\"token comment\"># [2022/06/16 16:24:31] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_img_11.jpg</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: Predict time of ./doc/imgs_en/img_12.jpg: 0.10506463050842285</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_img_12.jpg</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: Predict time of ./doc/imgs_en/img_195.jpg: 0.0799555778503418</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_img_195.jpg</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: Predict time of ./doc/imgs_en/model_prod_flow_en.png: 0.07767033576965332</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_model_prod_flow_en.png</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: Predict time of ./doc/imgs_en/wandb_metrics.png: 0.14113736152648926</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_wandb_metrics.png</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: Predict time of ./doc/imgs_en/wandb_models.png: 0.15174198150634766</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: The visualized image saved in ./inference_results/e2e_res_wandb_models.png</span>\n<span class=\"token comment\"># [2022/06/16 16:24:32] ppocr INFO: Avg Time: 0.10024774074554443</span></code></pre></div>\n<p>在识别的过程中，在另一个终端使用<code class=\"language-text\">nvidia-smi</code>，就能看到一个python任务正在占用GPU：</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">></span> nvidia-smi\n<span class=\"token comment\"># Thu Jun 16 16:24:30 2022       </span>\n<span class=\"token comment\"># +-----------------------------------------------------------------------------+</span>\n<span class=\"token comment\"># | NVIDIA-SMI 515.43.04    Driver Version: 516.01       CUDA Version: 11.7     |</span>\n<span class=\"token comment\"># |-------------------------------+----------------------+----------------------+</span>\n<span class=\"token comment\"># | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>\n<span class=\"token comment\"># | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>\n<span class=\"token comment\"># |                               |                      |               MIG M. |</span>\n<span class=\"token comment\"># |===============================+======================+======================|</span>\n<span class=\"token comment\"># |   0  NVIDIA GeForce ...  On   | 00000000:08:00.0  On |                  N/A |</span>\n<span class=\"token comment\"># |  0%   48C    P8    16W / 198W |   3412MiB /  8192MiB |     21%      Default |</span>\n<span class=\"token comment\"># |                               |                      |                  N/A |</span>\n<span class=\"token comment\"># +-------------------------------+----------------------+----------------------+</span>\n                                                                               \n<span class=\"token comment\"># +-----------------------------------------------------------------------------+</span>\n<span class=\"token comment\"># | Processes:                                                                  |</span>\n<span class=\"token comment\"># |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span>\n<span class=\"token comment\"># |        ID   ID                                                   Usage      |</span>\n<span class=\"token comment\"># |=============================================================================|</span>\n<span class=\"token comment\"># |    0   N/A  N/A      9129      C   /python3.9                      N/A      |</span></code></pre></div>\n<p>打开保存结果的目录<code class=\"language-text\">inference_results</code>就能看到识别的结果了\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; '>\n      <a class='gatsby-resp-image-link' href='/static/d2052b16c2796a9e33920a2a6f7a796a/ba579/e2e_res_img_12.jpg' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 62.28571428571429%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEEA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAGpVtIzcX//xAAZEAACAwEAAAAAAAAAAAAAAAAAEAECEhH/2gAIAQEAAQUCmhkwuL//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAZEAEBAAMBAAAAAAAAAAAAAAABABEhMUH/2gAIAQEAAT8h0Eb9bGAe2B5et//aAAwDAQACAAMAAAAQi/8A/8QAFhEBAQEAAAAAAAAAAAAAAAAAARAR/9oACAEDAQE/EEdn/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxABAAICAwAAAAAAAAAAAAAAAQARIUExYZH/2gAIAQEAAT8QAqszqXkU6UmRxuCFLnKBcX2n/9k='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='ocr_res' title='ocr_res' src='/static/d2052b16c2796a9e33920a2a6f7a796a/29d31/e2e_res_img_12.jpg' srcset='/static/d2052b16c2796a9e33920a2a6f7a796a/e52aa/e2e_res_img_12.jpg 175w,\n/static/d2052b16c2796a9e33920a2a6f7a796a/70ebb/e2e_res_img_12.jpg 350w,\n/static/d2052b16c2796a9e33920a2a6f7a796a/29d31/e2e_res_img_12.jpg 700w,\n/static/d2052b16c2796a9e33920a2a6f7a796a/9ecec/e2e_res_img_12.jpg 1050w,\n/static/d2052b16c2796a9e33920a2a6f7a796a/d165a/e2e_res_img_12.jpg 1400w,\n/static/d2052b16c2796a9e33920a2a6f7a796a/ba579/e2e_res_img_12.jpg 1680w' sizes='(max-width: 700px) 100vw, 700px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy'>\n  </a>\n    </span></p>\n<h1>6. It's all setup, happy deep learning!</h1>\n<p>最后别忘记安装VScode的<a href=\"https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-wsl\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">WSL remote插件</a>了, happy deep learning!</p>\n<h3>参考链接</h3>\n<ol>\n<li><a href=\"https://docs.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://docs.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl</a></li>\n<li><a href=\"https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started-with-cuda-on-wsl\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started-with-cuda-on-wsl</a></li>\n<li><a href=\"https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html</a></li>\n<li><a href=\"https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/linux-pip.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/linux-pip.html</a></li>\n<li><a href=\"https://www.jianshu.com/p/edaa744ea47d\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.jianshu.com/p/edaa744ea47d</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/463235082\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://zhuanlan.zhihu.com/p/463235082</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/83971195\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://zhuanlan.zhihu.com/p/83971195</a></li>\n</ol>","excerpt":"TL;DR PaddlePaddle是百度出品的深度学习框架。基于PaddlePaddle百度还推出了PaddleOCR，PaddleNLP，PaddleHub等实用的工具。作为深度学习框架，自然也在多个平台支持基于GPU的模型训练和推理，其中也包括Windows平台。如果你想使用Windows系统训练神经网络模型，而又想获得Linux的开发体验的话，Windows Subsystem for Linux ()显然是一个不错的选择。如今英伟达的官方显卡驱动官方支持了在WSL系统内部调用CUDA…","frontmatter":{"date":"June 13, 2022","path":"/cuda-gpu-setup-for-paddle-on-windows-wsl","title":"Windows环境下利用WSL搭建GPU训练/推理PaddlePaddle神经网络环境"}}},"pageContext":{}},"staticQueryHashes":["1176552510","3649515864","63159454","846684790"]}